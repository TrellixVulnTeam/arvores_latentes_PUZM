{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine Tuning - Predict2","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yqQyhsoaJqFe","colab_type":"text"},"source":["# Header"]},{"cell_type":"markdown","metadata":{"id":"fMKHndOWJsUK","colab_type":"text"},"source":["## Install"]},{"cell_type":"code","metadata":{"id":"n49bClA5Im33","colab_type":"code","outputId":"d4de84c7-dded-499a-fb60-6cc755769d36","executionInfo":{"status":"ok","timestamp":1565474065135,"user_tz":180,"elapsed":9259,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":615}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","!pip install bert-tensorflow\n","!pip install git+https://github.com/guillaumegenthial/tf_metrics.git\n","  \n","%load_ext tensorboard"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Collecting git+https://github.com/guillaumegenthial/tf_metrics.git\n","  Cloning https://github.com/guillaumegenthial/tf_metrics.git to /tmp/pip-req-build-5bnt28b2\n","  Running command git clone -q https://github.com/guillaumegenthial/tf_metrics.git /tmp/pip-req-build-5bnt28b2\n","Requirement already satisfied (use --upgrade to upgrade): tf-metrics==0.0.1 from git+https://github.com/guillaumegenthial/tf_metrics.git in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.16.4)\n","Requirement already satisfied: tensorflow-gpu>=1.6 in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.7.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.8.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.1.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.0.8)\n","Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.11.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.33.4)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.2)\n","Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (41.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.15.5)\n","Building wheels for collected packages: tf-metrics\n","  Building wheel for tf-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tf-metrics: filename=tf_metrics-0.0.1-cp36-none-any.whl size=7694 sha256=8e4f3d8db62300b4811b427c7a136976d834797e135afdef2dcc6603145a7f44\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-d3qx1tof/wheels/da/6c/c8/663ef339a0666590dc53bd13bab86643a1f9c35b26742d7876\n","Successfully built tf-metrics\n","The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6ONgBtOlJxpJ","colab_type":"text"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"JEPsT9Tf2SON","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","\n","from datetime import datetime\n","import tensorflow_hub as hub\n","import shutil\n","import sys\n","import pickle\n","import tf_metrics\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CT0gXiJnJy45","colab_type":"text"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"sdaMPFg9JbBW","colab_type":"code","colab":{}},"source":["hooks = []\n","debug = {}\n","\n","root = r\"/content/gdrive/My Drive/Puc/Projeto Final/\"\n","data_folder = f\"{root}/Datasets/finetuning/devel/\"\n","OUTPUT_DIR = f\"{root}/models/bert/devel_ok\"\n","\n","# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n","# Compute train and warmup steps from batch size\n","BATCH_SIZE = 4\n","MAX_EXAMPLES = 1\n","LEARNING_RATE = 2e-5\n","# Warmup is a period of time where hte learning rate\n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","MAX_SEQ_LEN = 512\n","num_classes = 768\n","pos_indices = list(range(1, num_classes))\n","average = 'micro'\n","features_path = sys.argv[-1]\n","MAX_CLASSES = 120\n","NON_ROOT_WEIGHT = 3\n","RANDOM_START = int(random.uniform(0,5))\n","min_loss = 1000\n","\n","tf.logging.set_verbosity(tf.logging.INFO)\n","#shutil.rmtree(OUTPUT_DIR, ignore_errors=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vs0FCzwDJ1Gh","colab_type":"text"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"UVoiG0YtJe-v","colab_type":"code","colab":{}},"source":["def build_estimator(labels):\n","    # Compute # train and warmup steps from batch size\n","    total_samples = sum([len(x) for x in labels])\n","    num_train_steps = int(total_samples * NUM_TRAIN_EPOCHS / BATCH_SIZE)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","        num_labels=0,\n","        learning_rate=LEARNING_RATE,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","        model_fn=model_fn,\n","        config=run_config,\n","        params={\"batch_size\": BATCH_SIZE})\n","\n","    return estimator\n","\n","\n","def load_features(path, max_size=10000):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    if os.path.isfile(\"all_features.dmp\"):\n","        with open(\"all_features.dmp\", \"rb\")as f:\n","            labels, tokens, ids = pickle.load(f)\n","    else:\n","        for file_name in tqdm(sorted(list(os.listdir(path)))):\n","            df = pd.read_csv(f\"{path}/{file_name}\", header=None, names=list(range(MAX_SEQ_LEN)), index_col=None)\n","\n","            if file_name.endswith(\"y\"):\n","                # Changing -1 and 0\n","                df = df.apply(lambda row: [-(x + 1) if x <= 0 else x for x in row])\n","                # Moving all 1 to get all positive\n","                labels.append(df + 1)\n","            elif file_name.endswith(\"x1\"):\n","                tokens.append(df)\n","            elif file_name.endswith(\"x2\"):\n","                ids.append(df)\n","\n","            if len(tokens) >= max_size and \\\n","                    len(labels) >= max_size and \\\n","                    len(ids) >= max_size:\n","                break\n","\n","        with open(\"all_features.dmp\", \"wb\") as f:\n","            pickle.dump((labels, tokens, ids), f)\n","\n","    masks = [k.sum(axis=1).values > 0 for k in ids]\n","    for i in range(len(masks)):\n","        labels[i] = labels[i][masks[i]]\n","        tokens[i] = tokens[i][masks[i]]\n","        ids[i] = ids[i][masks[i]]\n","\n","    labels = [labels[i] for i in range(len(masks)) if len(labels[i]) > 0]\n","    tokens = [tokens[i] for i in range(len(masks)) if len(tokens[i]) > 0]\n","    ids = [ids[i] for i in range(len(masks)) if len(ids[i]) > 0]\n","\n","    return tokens, ids, labels\n","\n","\n","def slice_input(epoch, o_tokens, o_ids, o_labels, train=True, skip_slice=False):\n","    print(\"Slicing\")\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    def slice(df):\n","        full = len(df) / 5\n","        chunk = int((epoch + RANDOM_START) % full)\n","        # Doing a 80-20 train-test split\n","        slice_start = chunk * 5\n","        slice_end = slice_start + 4\n","\n","        if not train:\n","            slice_start = slice_end\n","            slice_end = slice_start + 1\n","\n","        return df.iloc[slice_start:slice_end, :]\n","    if skip_slice:\n","      labels = o_labels\n","      tokens = o_tokens\n","      ids = o_ids\n","    else:\n","      for i in range(len(o_tokens)):\n","          labels.append(slice(o_labels[i]))\n","          tokens.append(slice(o_tokens[i]))\n","          ids.append(slice(o_ids[i]))\n","\n","    def fix_it(df):\n","        # print(f\"list values len {len(df)}\")\n","        # print(f\"shape 0 {df[0].shape}\")\n","        df = pd.concat(df)\n","        # print(f\"New df:{df.shape}\")\n","        return df.fillna(0).values.astype('int32')\n","\n","    sliced_labels = fix_it(labels)\n","    sliced_tokens = fix_it(tokens)\n","    sliced_ids = fix_it(ids)\n","    sliced_mask = (sliced_ids > 0).astype('int32')\n","\n","    sliced_loss_weight = (sliced_ids > 1) * (NON_ROOT_WEIGHT - 1)  # Will add 1 in row above\n","    sliced_loss_weight += sliced_mask\n","\n","    print(\"Sliced\")\n","    return input_fn_builder(sliced_tokens, sliced_ids, sliced_labels, sliced_mask, train, False, sliced_loss_weight)\n","\n","\n","def input_fn_builder(tokens, ids, labels, mask, is_training, drop_remainder, loss_weight):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    # print(f\"Tokens shape:{tokens.shape}\")\n","\n","    num_examples, seq_length = tokens.shape\n","\n","    # print(f\"t {num_examples}, {seq_length}\")\n","\n","    all_input_ids = tokens\n","    all_input_mask = mask\n","    all_segment_ids = ids\n","    all_label_ids = labels\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # This is for demo purposes and does NOT scale to large data sets. We do\n","        # not use Dataset.from_generator() because that uses tf.py_func which is\n","        # not TPU compatible. The right way to load data is with TFRecordReader.\n","        d = tf.data.Dataset.from_tensor_slices({\n","            \"input_ids\": all_input_ids,\n","            \"input_mask\": all_input_mask,\n","            \"segment_ids\": all_segment_ids,\n","            \"label_ids\": all_label_ids,\n","            \"loss_weight\": loss_weight\n","        })\n","\n","        if is_training:\n","            # d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","        d = d.prefetch(batch_size)\n","        return d\n","\n","    return input_fn\n","\n","\n","def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, loss_weight):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    bert_module = hub.Module(\n","        BERT_MODEL_HUB,\n","        trainable=True)\n","    bert_inputs = dict(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids)\n","    bert_outputs = bert_module(\n","        inputs=bert_inputs,\n","        signature=\"tokens\",\n","        as_dict=True)\n","\n","    one_hot_labels = tf.one_hot(labels, MAX_CLASSES, name=\"My_OneHot\")\n","\n","    # Use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"sequence_output\"]\n","    hidden_size = output_layer.shape[-1].value\n","\n","    output_weights = tf.get_variable(\n","        \"output_weights\", [MAX_CLASSES, hidden_size], \n","        initializer=tf.truncated_normal_initializer(mean=0.0, stddev=1))\n","\n","    output_bias = tf.get_variable(\n","        \"output_bias\", [MAX_SEQ_LEN, MAX_CLASSES], initializer=tf.zeros_initializer())\n","\n","    with tf.variable_scope(\"loss\"):\n","        output_layer_2 = tf.matmul(output_layer, output_weights, transpose_b=True)\n","        output_layer_2 = tf.add(output_layer_2, output_bias)\n","        # output_layer_2 = tf.layers.batch_normalization(output_layer_2, training=(not is_predicting))\n","        # output_layer_2 = tf.math.sigmoid(output_layer_2, name=\"My_output_layer\")\n","        output_layer_2 = tf.nn.relu(output_layer_2, name=\"My_output_layer\")\n","        \n","        # Dropout helps prevent overfitting\n","        # output_layer = tf.nn.dropout(output_layer, keep_prob=0.9, name=\"My_Dropout\")\n","        # print(f\"1 output_layer shape:{output_layer.shape}\")\n","\n","        # output_layer = tf.nn.softmax(output_layer, name=\"My Softmax\")\n","        # print(f\"2 output_layer shape:{output_layer.shape}\")\n","\n","        # tmp_mask = tf.broadcast_to(tf.expand_dims(input_mask, axis=-1), tf.shape(output_layer))\n","        # output_layer = tf.math.multiply(output_layer, tf.cast(tmp_mask, tf.float32), name=\"My_Argmax_Mask\")\n","\n","        # softmax = tf.nn.softmax(output_layer_2, name=\"My_Softmax\")\n","        argmax = tf.math.argmax(output_layer_2, axis=-1, name=\"My_Argmax\")\n","\n","        predicted_labels = tf.math.multiply(argmax, tf.cast(input_mask, tf.int64), name=\"d\")\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                # \"softmax\": softmax[0][0],\n","                # \"label\": labels[0],\n","                # \"argmax\": argmax[0][0],\n","                # \"segment_ids\": segment_ids[0],\n","                # \"output_layer\": output_layer[0][0],\n","                \"predicted\": predicted_labels[0],\n","                \"output_layer_2\": output_layer_2[0][0],\n","                \"output_weights\": output_weights[0],\n","                \"output_bias\": output_bias[0],\n","                # \"OK labels\": tf.math.reduce_sum(\n","                #    tf.to_float(tf.math.equal(tf.cast(labels, tf.int64), predicted_labels))),\n","                # \"Total Labels\": tf.shape(labels),\n","                # \"softmax shape\": tf.shape(softmax),\n","                # \"argmax shape\": tf.shape(argmax),\n","                # \"predicted_labels shape\": tf.shape(predicted_labels),\n","                # \"input_mask shape\": tf.shape(input_mask)\n","            }, every_n_iter=10))\n","\n","        if is_predicting:\n","            return predicted_labels\n","\n","        # output_layer = tf.reshape(output_layer, [BATCH_SIZE ,MAX_SEQ_LEN, 768])\n","        # print(f\"logits : {output_layer.shape}\")\n","        # print(f\"tmp : {tmp.shape}\")\n","\n","        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=output_layer_2, name=\"My_softmax_loss\")\n","        no_loss = tf.multiply(loss, tf.cast(input_mask, tf.float32), name=\"My_masked_loss\")\n","        weightened = tf.divide(no_loss, tf.cast(tf.reduce_sum(input_mask), tf.float32), name=\"My_wloss\")\n","        rm = tf.reduce_sum(weightened, name=\"My_final_loss\")\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                # \"count no_loss\": tf.math.count_nonzero(loss),\n","                # \"count input_mask\": tf.math.count_nonzero(input_mask),\n","                # \"loss shape\": tf.shape(loss),\n","                # \"rm shape\": tf.shape(rm),\n","                # \"no_loss shape\": tf.shape(no_loss),\n","                # \"one_hot_labels shape\": tf.shape(one_hot_labels),\n","                \"no_loss\": no_loss[0],\n","                # \"weightened\": weightened[0],\n","                \"loss_weight\": loss_weight[0],\n","                \"final_loss\": rm\n","            }, every_n_iter=10))\n","\n","        return rm, predicted_labels\n","\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","        loss_weight = features[\"loss_weight\"]\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","            print(\"Training\")\n","            (loss, predicted_labels) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, loss_weight)\n","            \n","            hooks.clear()\n","            hooks.append(tf.train.LoggingTensorHook({\"loss\": loss}, every_n_iter=10))\n","\n","            train_op = bert.optimization.create_optimizer(\n","                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            # Calculate evaluation metrics.\n","            def metric_fn(label_ids, predicted_labels):\n","                y_true, y_pred = label_ids, predicted_labels\n","\n","                precision = tf_metrics.precision(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                recall = tf_metrics.recall(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                f2 = tf_metrics.fbeta(\n","                    y_true, y_pred, num_classes, pos_indices, average=average, beta=2)\n","                f1 = tf_metrics.f1(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","\n","                return {\n","                    \"precision\": precision,\n","                    \"recall\": recall,\n","                    \"f2\": f2,\n","                    \"f1\": f1\n","                }\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  train_op=train_op,\n","                                                  training_hooks=hooks)\n","            else:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  eval_metric_ops=eval_metrics)\n","        else:\n","            print(\"Predicting\")\n","            predicted_labels = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, loss_weight)\n","\n","            return tf.estimator.EstimatorSpec(mode, predictions=predicted_labels)\n","\n","    # Return the actual model function in the closure\n","    return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AH0Wt_DpJ6Zh","colab_type":"text"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"BT2TQBBe_PO2","colab_type":"code","outputId":"c76a8a7e-2f4a-4ced-c201-c40ebaba2fad","executionInfo":{"status":"ok","timestamp":1565474109694,"user_tz":180,"elapsed":1231,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["current_time = datetime.now()\n","\n","tokens, ids, labels = load_features(data_folder, MAX_EXAMPLES)\n","\n","NUM_TRAIN_EPOCHS = int(max([len(x) for x in labels])/5)\n","print(f'Beginning Training! {NUM_TRAIN_EPOCHS}')\n","\n","estimator = build_estimator(labels)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  0%|          | 1/663 [00:00<02:05,  5.29it/s]\n","I0810 21:55:08.585765 140479516063616 estimator.py:209] Using config: {'_model_dir': '/content/gdrive/My Drive/Puc/Projeto Final//models/bert/devel_ok', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc37f0d9208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"},{"output_type":"stream","text":["Beginning Training! 20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ob-5r3Pz2X2P","colab_type":"code","outputId":"ecc2b186-1b65-4d42-e8cc-e494f1ba7afb","executionInfo":{"status":"ok","timestamp":1565474115808,"user_tz":180,"elapsed":579,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["predict_input_fn = slice_input(0, tokens, ids, labels, False, True)\n","predictions = estimator.predict(predict_input_fn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Slicing\n","Sliced\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mhgG_HVi3PW9","colab_type":"code","outputId":"56ed0750-a053-447e-ddb4-5335121c66a5","executionInfo":{"status":"ok","timestamp":1565474155595,"user_tz":180,"elapsed":39812,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["p = [x for x in predictions]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["I0810 21:55:16.213444 140479516063616 estimator.py:1145] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["Predicting\n"],"name":"stdout"},{"output_type":"stream","text":["I0810 21:55:29.261738 140479516063616 saver.py:1499] Saver not created because there are no variables in the graph to restore\n","I0810 21:55:29.512946 140479516063616 estimator.py:1147] Done calling model_fn.\n","W0810 21:55:29.662689 140479516063616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","I0810 21:55:30.131135 140479516063616 monitored_session.py:240] Graph was finalized.\n","W0810 21:55:31.400759 140479516063616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","I0810 21:55:31.407009 140479516063616 saver.py:1280] Restoring parameters from /content/gdrive/My Drive/Puc/Projeto Final//models/bert/devel_ok/model.ckpt-8290\n","I0810 21:55:44.203577 140479516063616 session_manager.py:500] Running local_init_op.\n","I0810 21:55:44.313647 140479516063616 session_manager.py:502] Done running local_init_op.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kyxWGYFvHtYb","colab_type":"code","outputId":"ccc6a77a-8c38-48bd-ed43-0cbc6c178cb8","executionInfo":{"status":"ok","timestamp":1565474239985,"user_tz":180,"elapsed":850,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(p[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"DZ7lKopRJSS_","colab_type":"code","outputId":"4370107b-dbe4-46f3-df3a-f2f6859c3e73","executionInfo":{"status":"ok","timestamp":1565474275760,"user_tz":180,"elapsed":608,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ids[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 512)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"eKhkoTXYJD9e","colab_type":"code","outputId":"baf1326c-867c-4d27-e994-0150c4ac6ff7","executionInfo":{"status":"ok","timestamp":1565474405000,"user_tz":180,"elapsed":590,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["import matplotlib.pyplot as plt\n","\n","_ = plt.imshow(labels[0])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAABkCAYAAACIC/vPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Ol0pMy4oHu3Z","colab_type":"code","outputId":"8aeca37c-b3e4-49b1-b283-389ec42dd5d2","executionInfo":{"status":"ok","timestamp":1565474417354,"user_tz":180,"elapsed":1040,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["_ = plt.imshow(ids[0])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAABkCAYAAACIC/vPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqegFxGJ3P8ARI2Cr9FbqAwAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"a2ztkHXANV_F","colab_type":"code","colab":{}},"source":["seq_epochs = list(range(NUM_TRAIN_EPOCHS))\n","random.shuffle(seq_epochs)\n","\n","for i,epoch in enumerate(seq_epochs):\n","    print(f\"New Epoch {i}\")\n","    train_input_fn = slice_input(epoch, tokens, ids, labels, True)\n","    eval_input_fn = slice_input(epoch, tokens, ids, labels, False)\n","\n","    #print(f\"hooks:{len(hooks)}\")\n","    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n","    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n","    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","\n","    # estimator.train(input_fn=slice_input(epoch, tokens, ids, labels, True), hooks=hooks)\n","    hooks.clear()\n","\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gb6VOQSoQ77","colab_type":"code","colab":{}},"source":["!cp -R BERT/eval /content/gdrive/My\\ Drive/Puc/Projeto\\ Final/models/bert/devel_ok/"],"execution_count":0,"outputs":[]}]}
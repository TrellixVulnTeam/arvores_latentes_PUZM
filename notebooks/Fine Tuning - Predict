{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine Tuning - Predict","version":"0.3.2","provenance":[],"collapsed_sections":["_68qIOsJ6JXb","Z1Mh6FEVZcpU"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Fhz3n0n-tlZp","colab_type":"text"},"source":["#Install"]},{"cell_type":"code","metadata":{"id":"JEPsT9Tf2SON","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","!pip install bert-tensorflow cort\n","!pip install git+https://github.com/guillaumegenthial/tf_metrics.git \n","  \n","!mkdir /content/bert_finetuning\n","\n","import nltk\n","nltk.download('wordnet')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tzo7TJwKQ30y","colab_type":"text"},"source":["!pip install git+https://github.com/guillaumegenthial/tf_metrics.git\n","!pip install tensorflow-gpu --force-reinstall"]},{"cell_type":"markdown","metadata":{"id":"7tTQRbtBZHvr","colab_type":"text"},"source":["## Paths"]},{"cell_type":"markdown","metadata":{"id":"gZrzFfNUtoNu","colab_type":"text"},"source":["# Pre-process data"]},{"cell_type":"code","metadata":{"id":"HqwixowI5cwR","colab_type":"code","colab":{}},"source":["root = r\"/content/gdrive/My Drive/Puc/Projeto Final/Datasets/\"\n","data_folder = \"train/\"\n","test_path = r\"/content/gdrive/My Drive/Puc/Projeto Final/Datasets/conll-formatted-ontonotes-5.0-master/conll-formatted-ontonotes-5.0/data/test.conll\"\n","\n","#For embedings\n","path_out = \"/content/bert_finetuning\"\n","path_in = f\"{root}/conll/test/\"\n","\n","#For Predicting\n","model_path = f\"{root}../models/bert/trained\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZtRZqrPZNqT","colab_type":"text"},"source":["## Imports"]},{"cell_type":"markdown","metadata":{"id":"lMZt7MgB2SXf","colab_type":"text"},"source":["### Mention Module"]},{"cell_type":"code","metadata":{"id":"LssA_gGR2SJW","colab_type":"code","colab":{}},"source":["\"\"\"\n","Mentions Module\n","\n","This module will handle the parse of the files and creation of a MentionsPair class that will be used to generate\n","the output files.\n","\"\"\"\n","\n","import difflib\n","import re\n","\n","from tqdm import tqdm\n","\n","CONLL_DOC_ID_COLUMN = 0\n","CONLL_PART_NUM_COLUMN = 1\n","CONLL_WORD_SEQ = 2\n","CONLL_WORD_COLUMN = 3\n","CONLL_POS_COLUMN = 4\n","CONLL_PARSE_BIT_COLUMN = 5\n","CONLL_LEMMA_COLUMN = 6\n","CONLL_FRAMESET_ID_COLUMN = 7\n","CONLL_SENSE_COLUMN = 8\n","CONLL_SPEAKER_COLUMN = 9\n","CONLL_NAMED_COLUMN = 10\n","\n","\n","class Mention:\n","    \"\"\"\n","    This class represents a mention with all its features. It can be extended if more information is needed\n","    \"\"\"\n","\n","    def __init__(self, m_id, words, start_pos, end_pos):\n","        self.words = words\n","        self.mention_id = m_id\n","        self.doc_id = m_id.split('_')[0]\n","        self.mention = \" \".join(words)\n","        self.first_word = words[0]\n","        self.last_word = words[-1]\n","        self.start_pos = start_pos\n","        self.end_pos = end_pos\n","\n","    def compare(self, other):\n","        start_diff = self.start_pos - other.start_pos\n","        if start_diff != 0:\n","            return start_diff\n","        return self.end_pos - other.end_pos\n","\n","    def __str__(self):\n","        return f\"Mention ({self.doc_id},{self.start_pos},{self.end_pos})\"\n","\n","\n","class MentionPair:\n","    \"\"\"\n","    This class represents a pair of mentions. It can be extended if more information is needed\n","    \"\"\"\n","\n","    def __init__(self, mention1, mention2):\n","        self.mention1 = mention1\n","        self.mention2 = mention2\n","        self.coref = mention1.mention_id == mention2.mention_id\n","\n","    def get_info_vector(self):\n","        \"\"\"\n","        Return the information to enable the recovery of the original mention pair\n","        :return: [start/end position for both mentions]\n","        \"\"\"\n","        return [self.mention1.start_pos, self.mention1.end_pos, self.mention2.start_pos, self.mention2.end_pos]\n","\n","\n","def check_usable_pairs(mention_list, i, j):\n","    \"\"\"\n","    Default implementation for checking if two mentions should be considered as a pair. These pairs are not necessarily\n","    correferences\n","    :param mention_list: list of mentions\n","    :param i: first index\n","    :param j: second index\n","    :return: True if mention i and mention j should be paired\n","    \"\"\"\n","    if mention_list[i].doc_id != mention_list[j].doc_id:\n","        return False\n","    if mention_list[i].mention_id == mention_list[j].mention_id:\n","        return True\n","    # Skipping some indexes so not all pairs will be generated\n","    if j % 2 == 0 or j % 3 == 0 or j % 5 == 0 or j % 7 == 0 or j % 11 == 0:\n","        return False\n","    return True\n","  \n","def build_mention_list_gold(train_list, fill_information=None):\n","    \"\"\"\n","    Build a list of dictionaries with the information about each mention. The mentions are not yet grouped\n","\n","    :param train_list: list of lines in the document\n","    :param fill_information: function to add more information into the cluster\n","    :return:\n","    \"\"\"\n","    mentions = []\n","    cluster_start, start_pos, cluster_end, end_pos = _get_mention(train_list)\n","    mention_cluster = _create_mention_cluster_list(cluster_start, start_pos, cluster_end, end_pos)\n","    for m in tqdm(mention_cluster, desc=\"mentions\"):\n","        m_id, start_pos, end_pos = m\n","\n","        mention_words = get_mention_words(train_list, start_pos, end_pos)\n","        mention = Mention(m_id, mention_words, start_pos, end_pos)\n","\n","        # Building features\n","        mention.mention_start = start_pos\n","        mention.mention_end = end_pos\n","        mention.pre_words = _get_preceding_words(train_list, start_pos)\n","        mention.next_words = _get_next_words(train_list, end_pos)\n","        mention.mention_sentence = _mention_sentence(train_list, start_pos)\n","        mention.speaker = train_list[start_pos - 1].split()[CONLL_SPEAKER_COLUMN]\n","\n","        # This will allow external info to be added\n","        if fill_information:\n","            fill_information(mention)\n","\n","        mentions.append(mention)\n","\n","    mentions = sorted(mentions, key=lambda k: k.mention_start)\n","    mentions = _check_mention_contain(mentions)\n","    mentions = _get_index(mentions)\n","    return mentions\n","\n","\n","def _add_extra_pair_info(mention_pair_list, train_list, increment_mention_pair=None):\n","    \"\"\"\n","    Adds distance information about the mention pairs.\n","    'overlap' : True (1) if the second element overlaps the first\n","    'speaker' : True (1) if both mentions have the same speaker\n","    'mention_exact_match' : True(1) if both mentions are from the same sentence\n","    'mention_partial_match' : True(1) if the sentences are similar\n","\n","    :param mention_pair_list: list of pair of mentions\n","    :param train_list: list of lines in the document\n","    :param increment_mention_pair: function to add more information to each pair\n","    :return:\n","    \"\"\"\n","    for p in mention_pair_list:\n","        p.overlap = p.mention1.overlap == p.mention2.mention_id\n","        p.speaker = p.mention1.speaker == p.mention2.speaker\n","        p.mention_exact_match = p.mention1.mention == p.mention2.mention\n","\n","        seq = difflib.SequenceMatcher(None, p.mention1.mention, p.mention2.mention)\n","        score = seq.ratio()\n","        p.mention_partial_match = score > 0.6\n","\n","        if increment_mention_pair:\n","            increment_mention_pair(p, train_list)\n","\n","    return mention_pair_list\n","\n","\n","def _get_mention(train_list):\n","    \"\"\"\n","    Creates four auxiliary lists with IDs and positions. These lists are not completely related to each other. It will\n","    simply gather the open/close parenthesis in the order it appears in the document.\n","    Check the formatting of a CoNLL document in the site\n","\n","        * Cluster start: ID of the opening parenthesis cluster\n","        * Start pos: line of starting position\n","        * Cluster end: ID of the closing parenthesis cluster (\n","        * End pos: line\n","\n","    :param train_list: List of lines of the file\n","    :return: cluster_start, start_pos, cluster_end, end_pos\n","    \"\"\"\n","    cluster_start = []\n","    start_pos = []\n","    cluster_end = []\n","    end_pos = []\n","    i = 1\n","    for line in train_list:\n","        if line == '\\n' or line == '-':  # Ignore empty lines\n","            i += 1\n","            continue\n","        part_number = line.split()[CONLL_PART_NUM_COLUMN]  # Part number column\n","        coref_col = line.split()[-1]  # Coref column\n","        for j in range(len(coref_col)):  # There can be more than one parenthesis. Tracking all of them\n","            if coref_col[j] == '(':\n","                cluster_start.append((str(part_number) + '_' + re.findall(r'\\d+', coref_col[j + 1:])[0]))\n","                start_pos.append(i)\n","            if coref_col[j] == ')':\n","                cluster_end.append((str(part_number) + '_' + re.findall(r'\\d+', coref_col[:j])[-1]))\n","                end_pos.append(i)\n","        i += 1\n","    return cluster_start, start_pos, cluster_end, end_pos\n","\n","\n","def _create_mention_cluster_list(cluster_start, start_pos, cluster_end, end_pos):\n","    \"\"\"\n","    Builds a list of mentions. One mention per item. The clusters are not yet grouped\n","     * First position is the id = [document_cluster]\n","     * Second position is the starting line\n","     * Third position in the ending line (counting from header).\n","     Use function get_mention to build the lists properly.\n","\n","    :param cluster_start: List of IDs of starting cluster IDs (opening parenthesis)\n","    :param start_pos: List of positions for the starting cluster ID\n","    :param cluster_end: List of IDs for ending cluster (closing parenthesis)\n","    :param end_pos: List of positions for the ending cluster ID\n","    :return: List with three items and length of the same size of the input lists\n","    \"\"\"\n","    cluster_start_end_list = []\n","    for start, pos in zip(cluster_start, start_pos):  # Join ID and position\n","        cluster = [start, pos]\n","        i = 0\n","        for i in range(len(cluster_end)):  # Search where is the closing parenthesis\n","            if cluster_end[i] == start:\n","                cluster.append(end_pos[i])  # Found it. Add to the pairing\n","                break\n","        del cluster_end[i]  # Remove from original list. This will prevent from being used again\n","        del end_pos[i]\n","        cluster_start_end_list.append(cluster)\n","    return cluster_start_end_list\n","\n","\n","def get_mention_words(train_list, pos1, pos2):\n","    \"\"\"\n","    Gets the list of words between these lines\n","    :param train_list: lines of the document\n","    :param pos1: initial line\n","    :param pos2: final line\n","    :return: List of all words\n","    \"\"\"\n","    mention = []\n","    for line_no in range(pos1 - 1, pos2):\n","        word = train_list[line_no].split()[CONLL_WORD_COLUMN]\n","        mention.append(word)\n","    return mention\n","\n","\n","def _get_preceding_words(train_list, pos, max_words=5):\n","    \"\"\"\n","    Get the previous max_words in the document (if they exists)\n","    :param train_list: list of lines\n","    :param pos: word position (numer of the line)\n","    :param max_words: max words to look ahead\n","    :return:\n","    \"\"\"\n","    word_part = train_list[pos - 1].split()[CONLL_PART_NUM_COLUMN]\n","    num_words = 0\n","    word = []\n","    for i in range(2, pos):\n","        if train_list[pos - i] != '\\n':\n","            doc_id = train_list[pos - i].split()[CONLL_DOC_ID_COLUMN]  # Checking if begin/end of document was reached\n","            if doc_id == '#begin' or doc_id == '#end':\n","                break\n","            part_no = train_list[pos - i].split()[CONLL_PART_NUM_COLUMN]\n","            if part_no == word_part:  # Will add only words for the same part\n","                word.append(train_list[pos - i].split()[CONLL_WORD_COLUMN])\n","                num_words += 1\n","            if num_words == max_words:\n","                break\n","    return word\n","\n","\n","def _get_next_words(train_list, pos, max_words=5):\n","    \"\"\"\n","    Get the next max_words in the document (if they exists)\n","    :param train_list: list of lines\n","    :param pos: word position (numer of the line)\n","    :param max_words: max words to look ahead\n","    :return:\n","    \"\"\"\n","    pos = pos - 1\n","    word_part = train_list[pos].split()[CONLL_PART_NUM_COLUMN]\n","    num_words = 0\n","    word = []\n","    for i in range(1, len(train_list) - pos):\n","        if train_list[pos + i] != '\\n':\n","            doc_id = train_list[pos + i].split()[CONLL_DOC_ID_COLUMN]  # Checking if begin/end of document was reached\n","            if doc_id == '#begin' or doc_id == '#end':\n","                break\n","            part_no = train_list[pos + i].split()[CONLL_PART_NUM_COLUMN]\n","            if part_no == word_part:\n","                word.append(train_list[pos + i].split()[CONLL_WORD_COLUMN])\n","                num_words += 1\n","            if num_words == max_words:\n","                break\n","        i += 1\n","    return word\n","\n","\n","def _mention_sentence(train_list, pos):\n","    \"\"\"\n","    Gets the sentence that contains the mention in this line\n","    :param train_list: list of lines\n","    :param pos: line of reference\n","    :return: string with sentence\n","    \"\"\"\n","    pos = pos - 1\n","    i = 1\n","    end = 0\n","    while True:\n","        if train_list[pos - i] == '\\n':\n","            start = pos - i\n","            break\n","        if train_list[pos - i].split()[CONLL_DOC_ID_COLUMN] == '#begin':\n","            start = pos - i\n","            break\n","        i += 1\n","    start += 2\n","    i = 1\n","\n","    while True:\n","        if train_list[pos + i] == '\\n':\n","            end = pos + i\n","            break\n","        if train_list[pos + i].split()[CONLL_DOC_ID_COLUMN] == '#end':\n","            start = pos + i\n","            break\n","        i += 1\n","    sentence = get_mention_words(train_list, start, end)\n","    return \" \".join(sentence)\n","\n","\n","def _check_mention_contain(mention_list):\n","    \"\"\"\n","    Adds information about mentions containing/overlapping other mentions. The difference between contain and overlap\n","    will be defined by the end position. In this context, overlapping is not reflexive.\n","    The keys 'contained'/'overlap' will have the id of the container/overlapped mention or False if it is not contained\n","\n","    :param mention_list: list of dictionaries, one item per mention. Each item must have the keys:\n","        mention_start. mention_end and id\n","\n","    :return: the modified list\n","    \"\"\"\n","    for i in range(0, len(mention_list)):\n","        start = mention_list[i].mention_start\n","        end = mention_list[i].mention_end\n","\n","        for j in range(0, len(mention_list)):\n","            c_start = mention_list[j].mention_start\n","            c_end = mention_list[j].mention_end\n","\n","            if c_start == start and c_end == end:\n","                continue\n","\n","            if c_start >= start:  # If starts after the ref\n","                if c_end <= end:  # and end before, it is contained by the ref\n","                    mention_list[j].contained = mention_list[i].mention_id\n","                if c_start <= end:  # if starts before the end of the ref, only overlaps\n","                    mention_list[j].overlap = mention_list[i].mention_id\n","\n","    for k in range(0, len(mention_list)):  # Filling missing info\n","        if not hasattr(mention_list[k], 'contained'):\n","            mention_list[k].contained = False\n","        if not hasattr(mention_list[k], 'overlap'):\n","            mention_list[k].overlap = False\n","\n","    return mention_list\n","\n","\n","def _get_index(mentions):\n","    \"\"\"\n","    Adds the 'index' and 'mention_position'.\n","    'index' is a counter for each mention in each document\n","    'mention_position' is the index in a [0,1] interval (0 is the first, 1 is the last).\n","    :param mentions: list of all mentions\n","    :return: the same list of input, with objects changed\n","    \"\"\"\n","    doc_count = '0'\n","    count = 0\n","    i = 0\n","    mentions_in_each_doc = []\n","    for m in mentions:\n","        if m.doc_id == doc_count:  # Keep counting while it's the same doc\n","            count += 1\n","        else:\n","            mentions_in_each_doc.append(count)  # save the counter\n","            doc_count = m.doc_id\n","            count = 1\n","        m.index = count\n","    mentions_in_each_doc.append(count)  # save last counter\n","\n","    # Now will transform into [0,1,...]\n","    doc_count = '0'\n","    for m in mentions:\n","        if m.doc_id == doc_count:\n","            m.mention_position = m.index / mentions_in_each_doc[i]\n","        else:\n","            doc_count = m.doc_id\n","            i += 1\n","            m.mention_position = m.index / mentions_in_each_doc[i]\n","\n","    return mentions\n","\n","\n","def get_mention_pairs(train_list, increment_mention_info=None, increment_mention_pair=None,\n","                      use_pair=check_usable_pairs):\n","    \"\"\"\n","    Builds a list of pair of mentions for the file. Each pair may or may not have a coreference. Each position\n","    simulates an object with the first two positions being mentions and the following are dictionaries with extra\n","    features\n","\n","    :param train_list: list of lines in the file\n","    :param increment_mention_info: function to add more information to the mention\n","    :param increment_mention_pair: function to add more information to the mention pair\n","    :param use_pair: function to define if two mentions should be paired or not\n","    :return: list of objects\n","    \"\"\"\n","    mention_list = build_mention_list(train_list, increment_mention_info)\n","    mention_pair_list = []\n","    for i in tqdm(range(1, len(mention_list)), desc=\"mention pair\"):\n","        for j in range(0, i):\n","            if use_pair(mention_list, i, j):\n","                pair = MentionPair(mention_list[i], mention_list[j])\n","                mention_pair_list.append(pair)\n","\n","    # Adding extra info\n","    mention_pair_list = _add_extra_pair_info(mention_pair_list, train_list, increment_mention_pair)\n","\n","    return mention_pair_list\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_68qIOsJ6JXb","colab_type":"text"},"source":["### Loader Module"]},{"cell_type":"code","metadata":{"id":"OsjSbnpF6Lmp","colab_type":"code","colab":{}},"source":["\"\"\"\n","This module will parse the files in the CoNLL format.\n","The main function is trainfile_to_vectors\n","\n","\"\"\"\n","\n","import os\n","import re\n","\n","from tqdm import tqdm\n","\n","\n","def trainfile_to_vectors(path, increment_mention, increment_mention_pair, make_vectors):\n","    \"\"\"\n","    Given one file, returns the input and output vectors to be passed to a learning algo\n","    :param path: file path to be used\n","    :param increment_mention: method to add information to the mention\n","    :param increment_mention_pair: method to add information to the mention pair\n","    :param make_vectors: method to build the vectors\n","    :return: [input_vector, output_vector, document_name]\n","    \"\"\"\n","    train_list = train_file_to_list(path)\n","    pairs = get_mention_pairs(train_list, increment_mention, increment_mention_pair)\n","    input_vector, output_vector = make_vectors(pairs, train_list=train_list)\n","    input_vector = _append_mention_info(pairs, input_vector)\n","    return input_vector, output_vector, _get_document_name(train_list)\n","\n","\n","def process_dir(path_in, path_out, callback):\n","    with tqdm(os.walk(path_in), desc=\"folders\")as pb:\n","        for r, d, f in pb:\n","            pb.set_description(\"folder:...{}\".format(r[-20:]))\n","            for file_name in tqdm(f, desc=\"files\"):\n","                if not \"gold\" in file_name:\n","                    continue\n","                v_in, v_out, doc_name = callback(os.path.join(r, file_name))\n","\n","                if len(v_in) > 0 and len(v_out) > 0:\n","                    _save_to_file(v_in, path_out, file_name + \"_in\", doc_name)\n","                    _save_to_file(v_out, path_out, file_name + \"_out\")\n","\n","\n","def transform_conll_to_vectors(path_in, path_out, increment_mention, increment_mention_pair, make_vectors):\n","    \"\"\"\n","    Walks the input path looking for *_conll files. If any file is found, it is processed and two files are generated\n","    into the path_out root.\n","    A file with pattern [original_name]_in with the input vectors and [original_name]_out with the output vectors\n","    :param path_in: root folder to be searched. Files can be in multiple sub-folders\n","    :param path_out: output folder\n","    :param increment_mention: method to add information to the mention\n","    :param increment_mention_pair: method to add information to the mention pair\n","    :param make_vectors: method to build the vectors\n","    \"\"\"\n","\n","    process_dir(path_in, path_out,\n","                lambda x: trainfile_to_vectors(x, increment_mention, increment_mention_pair, make_vectors))\n","\n","\n","def train_file_to_list(file):\n","    \"\"\"\n","    :param file: file name to be read\n","    :return: list of all lines\n","    \"\"\"\n","    with open(file, \"r\", encoding=\"utf8\") as f:\n","        return f.readlines()\n","\n","\n","def _save_to_file(vector, path, file_name, doc_name=None):\n","    \"\"\"\n","    Saves a vector into a file if the vector is not empty\n","    :param vector: list of lists of values. Can be numpy arrays\n","    :param path: folder to save\n","    :param file_name: file name to use\n","    \"\"\"\n","    if len(vector) == 0:  # Do not create empty files\n","        return\n","    with open(os.path.join(path, file_name), \"w\") as f:\n","        if doc_name:\n","            f.write(doc_name + \"\\n\")\n","\n","        for line in vector:\n","            f.write(\",\".join([str(i) for i in line]) + \"\\n\")\n","\n","\n","def _append_mention_info(pairs, input_vectors):\n","    \"\"\"\n","    Append pair information into the vector to be saved in the disk\n","    :param pairs: list of mention pairs\n","    :param input_vectors: list of vectors\n","    :return: a list of all information appended\n","    \"\"\"\n","    appended = []\n","    for i in range(len(pairs)):\n","        vec_as_list = list([x[0] for x in input_vectors[i]])\n","        if len(vec_as_list) == 0:\n","            continue\n","        appended.append(list(pairs[i].get_info_vector()) + vec_as_list)\n","\n","    return appended\n","\n","\n","def _get_document_name(train_list):\n","    \"\"\"\n","    The the document name from the first line.\n","    :param train_list: list of all lines in the document\n","    :return: the document name\n","    \"\"\"\n","    name_re = re.compile(r\".*\\((.*)\\).*\")\n","    match = name_re.match(train_list[0])\n","    return match.group(1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-UMlkTB42x6","colab_type":"text"},"source":["### Creating DataSet"]},{"cell_type":"code","metadata":{"id":"vNe3WtE7451w","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from tqdm import tqdm\n","import random\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import bert\n","from bert import tokenization\n","\n","MAX_SEQ_LEN = 512\n","\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","\n","class Span:\n","    def __init__(self, tokenizer, start_pos, words):\n","        self.start_pos = start_pos\n","        self.words = words\n","        self.end_pos = start_pos + len(words) - 1\n","        self.tokens = [tokenizer.tokenize(x) for x in words]\n","        self.tokens = [item for sublist in self.tokens for item in sublist]\n","\n","        assert self.token_len() <= MAX_SEQ_LEN\n","\n","    def token_len(self):\n","        return len(self.tokens)\n","\n","    def __str__(self):\n","        return f\"({self.start_pos}, {self.end_pos})\"\n","\n","\n","class Cluster:\n","    def __init__(self):\n","        self.first_mention = None  # The first occurring mention\n","        self.mentions = []  # All other mentions in cluster\n","\n","    def add_mention(self, mention):\n","        if mention in self.mentions:\n","            return\n","\n","        if self.first_mention is None:  # First Mention\n","            self.first_mention = mention\n","        elif self.first_mention.compare(mention) > 0:\n","            self.mentions.append(self.first_mention)  # Save the mention that was the first\n","            self.first_mention = mention  # Set the new one\n","        else:\n","            self.mentions.append(mention)  # Just save the mention\n","\n","\n","class Line:\n","    def __init__(self, line):\n","        self.line = line\n","        self.word = None\n","        if isinstance(line, str):\n","            if (\"#begin\" in line) or (\"#end\" in line):\n","                return\n","            split = line.split()\n","            if len(split) >= CONLL_WORD_COLUMN:\n","                self.word = split[CONLL_WORD_COLUMN]\n","\n","    def __str__(self):\n","        return self.line\n","\n","\n","class TreeNode:\n","    def __init__(self, mention, parent=None):\n","        self.mention = mention\n","        self.children = []\n","        self.parent = parent\n","        if self.parent:\n","            self.parent.add_node(self)\n","\n","    def add_node(self, node):\n","        self.children.append(node)\n","        node.parent = self\n","\n","    def add_mention(self, mention):\n","        node = TreeNode(mention)\n","        self.add_node(node)\n","        return node\n","\n","    def delete(self):\n","        for c in self.children:\n","            self.parent.add_node(c)\n","\n","        try:\n","            self.parent.children.remove(self)\n","        except ValueError:\n","            pass\n","\n","    def __str__(self):\n","        return f\"TreeNode ({self.parent}, {self.mention})\"\n","\n","\n","class Tree:\n","    def __init__(self):\n","        self.root = TreeNode(None)\n","        self.mention_map = {}\n","\n","    def add(self, mention, parent=None):\n","        if parent is None:\n","            return self.add(mention, self.root)\n","\n","        return TreeNode(mention, parent)\n","\n","    def get_parent_id(self, mention):\n","        if mention not in self.mention_map:\n","            print(\"Not in mapping\")\n","            return 0\n","\n","        node = self.mention_map[mention]\n","        parent = node.parent\n","        if parent is None or parent.mention is None:\n","            return -1\n","        return parent.mention.seq_id\n","\n","    def update_mapping(self, new_map):\n","        self.mention_map.update(new_map)\n","\n","    def remove_mention(self, mention):\n","        node = self.mention_map[mention]\n","        node.delete()\n","\n","\n","class SuperStructure:\n","    def __init__(self, tokenizer, lines):\n","        self.structure_map = self.build_map(tokenizer, lines)\n","\n","    def build_map(self, tokenizer, lines):\n","        spans = self.detect_spans(tokenizer, lines)\n","        spans = self.group_spans(spans, MAX_SEQ_LEN)\n","        return {x: self.build_substructure(tokenizer, lines, x) for x in spans}\n","\n","    def add_mention(self, mention):\n","        for span, structure in self.structure_map.items():\n","            if span[0] <= mention.start_pos and span[1] >= mention.end_pos:\n","                structure.add_mention(mention)\n","\n","    def get_random_example(self):\n","        all_x1 = []\n","        all_x2 = []\n","        all_y = []\n","        for structure in self.structure_map.values():\n","            x1, x2, y = structure.get_random_example()\n","            all_x1.append(x1)\n","            all_x2.append(x2)\n","            all_y.append(y)\n","        return all_x1, all_x2, all_y\n","\n","    @staticmethod\n","    def build_substructure(tokenizer, lines, span):\n","        return Structure(tokenizer, lines[span[0]:(span[1] + 1)], span[0])\n","\n","    @staticmethod\n","    def group_spans(spans, max_length):\n","        total_length = 0\n","        groups = []\n","        current_group = []\n","\n","        def current_rep():\n","            start = current_group[0].start_pos\n","            end = current_group[-1].end_pos\n","            return start, end\n","\n","        for span in spans:\n","            inc = span.token_len() + 1\n","            if total_length + inc > max_length:\n","                groups.append(current_rep())\n","\n","                while total_length + inc > (max_length / 2) and len(current_group) > 0:\n","                    total_length -= current_group.pop(0).token_len() + 1\n","\n","            current_group.append(span)\n","            total_length += inc\n","            # print(f\"total_length:{total_length}\")\n","\n","            assert total_length <= max_length, f\"New length:{total_length}\"\n","\n","        if len(current_group) > 0:  # Last sentences\n","            assert sum([x.token_len() for x in current_group]) <= max_length\n","            groups.append(current_rep())\n","\n","        return groups\n","\n","    @staticmethod\n","    def detect_spans(tokenizer, lines):\n","        spans = []\n","        running = False\n","        start = 0\n","        words = []\n","        for idx, line in enumerate(lines):\n","            line = line.strip(\"\\n\").strip(\"\\r\")\n","            if \"#begin\" in line:\n","                continue\n","            if len(line) == 0 or \"#end\" in line:\n","                running = False\n","                spans.append(Span(tokenizer, start, words))\n","                words = []\n","                continue\n","            if not running:\n","                start = idx\n","                running = True\n","            words.append(line.split()[CONLL_WORD_COLUMN])\n","\n","        return spans\n","\n","    def __len__(self):\n","        return sum([len(s) for s in self.structure_map.values()])\n","\n","\n","class Structure:\n","    def __init__(self, tokenizer, lines, first_position):\n","        self.lines = [Line(x) for x in lines]\n","        self.clusters = {}\n","        self.flat = [[] for x in range(len(lines))]\n","        self.first_position = first_position\n","        self.tokens = [[] if l.word is None else tokenizer.tokenize(l.word) for l in self.lines]\n","        self.tokens = [tokenizer.convert_tokens_to_ids(t) for t in self.tokens]\n","\n","        if sum([len(x) for x in self.tokens]) > MAX_SEQ_LEN:\n","            print(\"Too much\")\n","\n","    def __len__(self):\n","        return sum([len(x) for x in self.flat])\n","\n","    def add_mention(self, mention):\n","        mention_id = mention.mention_id\n","\n","        if mention_id not in self.clusters:\n","            self.clusters[mention_id] = Cluster()\n","\n","        self.clusters[mention_id].add_mention(mention)\n","        self.flat[mention.start_pos - self.first_position].append(mention)\n","\n","    def get_random_example(self):\n","        tree = self.get_random_tree()\n","        ids, parents = self.build_vectors(tree)\n","\n","        X1 = []\n","        X2 = []\n","        Y = []\n","\n","        for index, line in enumerate(self.lines):\n","            # if line.word is None:\n","            #    continue\n","            index_id = ids[index]\n","            parent_id = parents[index]\n","            for tk in self.tokens[index]:\n","                X1.append(tk)\n","                X2.append(index_id)\n","                Y.append(parent_id)\n","\n","        assert len(Y) <= MAX_SEQ_LEN, f\"Too big {len(Y)}, {self.first_position}\"\n","        return X1, X2, Y\n","\n","    def build_vectors(self, tree):\n","        ids = np.zeros(len(self.flat))\n","        parents = np.zeros(len(self.flat))\n","\n","        counter = 1\n","        for i in range(len(ids)):\n","            mentions = self.flat[i]\n","            if len(mentions) == 0:\n","                continue\n","            mentions.sort(key=lambda x: x.end_pos)\n","            for mention in mentions:\n","                # mention = self.clean_inconsistency(mentions, tree)\n","                end_pos = mention.end_pos + 1\n","                ids[mention.start_pos: end_pos] = counter\n","                parents[mention.start_pos: end_pos] = tree.get_parent_id(mention)\n","                mention.seq_id = counter\n","                counter += 1\n","        return ids, parents\n","\n","    def clean_inconsistency(self, mentions, tree):\n","        survivor = random.choice(mentions)\n","        for m in mentions:\n","            if m == survivor:\n","                continue\n","            tree.remove_mention(m)\n","        return survivor\n","\n","    def get_random_tree(self):\n","        tree = Tree()\n","        for cluster in self.clusters.values():\n","            subtree, mapping = self.get_random_subtree(cluster)\n","            tree.root.add_node(subtree)\n","            tree.update_mapping(mapping)\n","        return tree\n","\n","    @staticmethod\n","    def get_random_subtree(cluster):\n","        mapping = {}\n","        root = TreeNode(cluster.first_mention)\n","        mapping[cluster.first_mention] = root\n","\n","        max_height = 0\n","        for mention in cluster.mentions:\n","            next_root = root\n","            height = 1\n","            last = round(random.random() * max_height)\n","            for i in range(last):\n","                # only descent in tree in children that occur before the current mention\n","                possible_children = [x for x in next_root.children if x.mention.compare(mention) < 0]\n","\n","                if len(possible_children) == 0:\n","                    break\n","                next_root = random.choice(possible_children)\n","                height += 1\n","            node = next_root.add_mention(mention)\n","            mapping[mention] = node\n","\n","            if height >= max_height:\n","                max_height = height\n","\n","        return root, mapping\n","\n","\n","def build_structure(tokenizer, lines):\n","    structure = SuperStructure(tokenizer, lines)\n","\n","    for mention in build_mention_list(lines):\n","        structure.add_mention(mention)\n","\n","    return structure\n","\n","\n","def to_vectors(file_path, num_examples, tokenizer):\n","    lines = train_file_to_list(file_path)\n","    structure = build_structure(tokenizer, lines)\n","\n","    X1 = []\n","    X2 = []\n","    Y = []\n","\n","    if len(structure) == 0:\n","        return X1, X2, Y\n","\n","    found = set()\n","    for i in range(num_examples * 3):\n","        x1, x2, y = structure.get_random_example()\n","        ex_id = sum([sum(k) for k in y])\n","        if ex_id in found:\n","            continue\n","        found.add(ex_id)\n","        X1 += x1\n","        X2 += x2\n","        Y += y\n","        if len(found) >= num_examples:\n","            break\n","\n","    return X1, X2, Y\n","\n","\n","def create_tokenizer_from_hub_module():\n","    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","    with tf.Graph().as_default():\n","        bert_module = hub.Module(BERT_MODEL_HUB)\n","        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","        with tf.Session() as sess:\n","            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                                  tokenization_info[\"do_lower_case\"]])\n","\n","    return bert.tokenization.FullTokenizer(\n","        vocab_file=vocab_file, do_lower_case=do_lower_case)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5trD6-r-shG","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from tqdm import tqdm\n","import random\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import bert\n","from bert import tokenization\n","\n","MAX_SEQ_LEN = 512\n","\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","\n","class Span:\n","    def __init__(self, tokenizer, start_pos, words):\n","        self.start_pos = start_pos\n","        self.words = words\n","        self.end_pos = start_pos + len(words) - 1\n","        self.tokens = [tokenizer.tokenize(x) for x in words]\n","        self.tokens = [item for sublist in self.tokens for item in sublist]\n","\n","        assert self.token_len() <= MAX_SEQ_LEN\n","\n","    def token_len(self):\n","        return len(self.tokens)\n","\n","    def __str__(self):\n","        return f\"({self.start_pos}, {self.end_pos})\"\n","\n","\n","class Cluster:\n","    def __init__(self):\n","        self.first_mention = None  # The first occurring mention\n","        self.mentions = []  # All other mentions in cluster\n","\n","    def add_mention(self, mention):\n","        if mention in self.mentions:\n","            return\n","\n","        if self.first_mention is None:  # First Mention\n","            self.first_mention = mention\n","        elif self.first_mention.compare(mention) > 0:\n","            self.mentions.append(self.first_mention)  # Save the mention that was the first\n","            self.first_mention = mention  # Set the new one\n","        else:\n","            self.mentions.append(mention)  # Just save the mention\n","\n","\n","class Line:\n","    def __init__(self, line):\n","        self.line = line\n","        self.word = None\n","        if isinstance(line, str):\n","            if (\"#begin\" in line) or (\"#end\" in line):\n","                return\n","            split = line.split()\n","            if len(split) >= CONLL_WORD_COLUMN:\n","                self.word = split[CONLL_WORD_COLUMN]\n","\n","    def __str__(self):\n","        return self.line\n","\n","\n","class TreeNode:\n","    def __init__(self, mention, parent=None):\n","        self.mention = mention\n","        self.children = []\n","        self.parent = parent\n","        if self.parent:\n","            self.parent.add_node(self)\n","\n","    def add_node(self, node):\n","        self.children.append(node)\n","        node.parent = self\n","\n","    def add_mention(self, mention):\n","        node = TreeNode(mention)\n","        self.add_node(node)\n","        return node\n","\n","    def delete(self):\n","        for c in self.children:\n","            self.parent.add_node(c)\n","\n","        try:\n","            self.parent.children.remove(self)\n","        except ValueError:\n","            pass\n","\n","    def __str__(self):\n","        return f\"TreeNode ({self.parent}, {self.mention})\"\n","\n","\n","class Tree:\n","    def __init__(self):\n","        self.root = TreeNode(None)\n","        self.mention_map = {}\n","\n","    def add(self, mention, parent=None):\n","        if parent is None:\n","            return self.add(mention, self.root)\n","\n","        return TreeNode(mention, parent)\n","\n","    def get_parent_id(self, mention):\n","        if mention not in self.mention_map:\n","            print(\"Not in mapping\")\n","            return 0\n","\n","        node = self.mention_map[mention]\n","        parent = node.parent\n","        if parent is None or parent.mention is None:\n","            return -1\n","        return parent.mention.seq_id\n","\n","    def update_mapping(self, new_map):\n","        self.mention_map.update(new_map)\n","\n","    def remove_mention(self, mention):\n","        node = self.mention_map[mention]\n","        node.delete()\n","\n","\n","class SuperStructure:\n","    def __init__(self, tokenizer, lines):\n","        self.structure_map = self.build_map(tokenizer, lines)\n","\n","    def build_map(self, tokenizer, lines):\n","        all_groups = []\n","        for start, end in self.find_doc_spans(lines):\n","            spans = self.detect_spans(tokenizer, lines[start:end])\n","            span_grouped = self.group_spans(spans, MAX_SEQ_LEN)\n","            all_groups += [(x[0] + start, x[1] + start) for x in span_grouped]\n","\n","        return {x: self.build_substructure(tokenizer, lines, x) for x in all_groups}\n","\n","    @staticmethod\n","    def find_doc_spans(lines):\n","        start = -1\n","        for idx, line in enumerate(lines):\n","            if \"#begin\" in line:\n","                start = idx\n","            elif \"#end\" in line:\n","                yield (start, idx + 1)\n","\n","    def add_mention(self, mention):\n","        for span, structure in self.structure_map.items():\n","            if span[0] <= mention.start_pos and span[1] >= mention.end_pos:\n","                structure.add_mention(mention)\n","\n","    def get_random_example(self):\n","        all_x1 = []\n","        all_x2 = []\n","        all_y = []\n","        for structure in self.structure_map.values():\n","            x1, x2, y = structure.get_random_example()\n","            all_x1.append(x1)\n","            all_x2.append(x2)\n","            all_y.append(y)\n","        return all_x1, all_x2, all_y\n","\n","    @staticmethod\n","    def build_substructure(tokenizer, lines, span):\n","        return Structure(tokenizer, lines[span[0]:(span[1] + 1)], span[0])\n","\n","    @staticmethod\n","    def group_spans(spans, max_length):\n","        total_length = 0\n","        groups = []\n","        current_group = []\n","\n","        def current_rep():\n","            start = current_group[0].start_pos\n","            end = current_group[-1].end_pos\n","            return start, end\n","\n","        for span in spans:\n","            inc = span.token_len() + 1\n","            if total_length + inc > max_length:\n","                groups.append(current_rep())\n","\n","                while total_length + inc > (max_length / 2) and len(current_group) > 0:\n","                    total_length -= current_group.pop(0).token_len() + 1\n","\n","            current_group.append(span)\n","            total_length += inc\n","\n","            assert total_length <= max_length, f\"New length:{total_length}\"\n","\n","        if len(current_group) > 0:  # Last sentences\n","            assert sum([x.token_len() for x in current_group]) <= max_length\n","            groups.append(current_rep())\n","\n","        return groups\n","\n","    @staticmethod\n","    def detect_spans(tokenizer, lines):\n","        spans = []\n","        running = False\n","        start = 0\n","        words = []\n","        for idx, line in enumerate(lines):\n","            line = line.strip(\"\\n\").strip(\"\\r\")\n","            if \"#begin\" in line:\n","                continue\n","            if len(line) == 0 or \"#end\" in line:\n","                if running:\n","                    running = False\n","                    spans.append(Span(tokenizer, start, words))\n","                    words = []\n","                continue\n","            if not running:\n","                start = idx\n","                running = True\n","            words.append(line.split()[CONLL_WORD_COLUMN])\n","\n","        return spans\n","\n","    def __len__(self):\n","        return sum([len(s) for s in self.structure_map.values()])\n","\n","\n","class Structure:\n","    def __init__(self, tokenizer, lines, first_position):\n","        self.lines = [Line(x) for x in lines]\n","        self.clusters = {}\n","        self.flat = [[] for x in range(len(lines))]\n","        self.first_position = first_position\n","        self.tokens = [[] if l.word is None else tokenizer.tokenize(l.word) for l in self.lines]\n","        self.tokens = [tokenizer.convert_tokens_to_ids(t) for t in self.tokens]\n","\n","        if sum([len(x) for x in self.tokens]) > MAX_SEQ_LEN:\n","            print(\"Too much\")\n","\n","    def __len__(self):\n","        return sum([len(x) for x in self.flat])\n","\n","    def add_mention(self, mention):\n","        mention_id = mention.mention_id\n","\n","        if mention_id not in self.clusters:\n","            self.clusters[mention_id] = Cluster()\n","\n","        self.clusters[mention_id].add_mention(mention)\n","        self.flat[mention.start_pos - self.first_position].append(mention)\n","\n","    def get_random_example(self):\n","        tree = self.get_random_tree()\n","        ids, parents = self.build_vectors(tree)\n","\n","        X1 = []\n","        X2 = []\n","        Y = []\n","\n","        for index, line in enumerate(self.lines):\n","            # if line.word is None:\n","            #    continue\n","            index_id = ids[index]\n","            parent_id = parents[index]\n","            for tk in self.tokens[index]:\n","                X1.append(tk)\n","                X2.append(index_id)\n","                Y.append(parent_id)\n","\n","        assert len(Y) <= MAX_SEQ_LEN, f\"Too big {len(Y)}, {self.first_position}\"\n","        return X1, X2, Y\n","\n","    def build_vectors(self, tree):\n","        ids = np.zeros(len(self.flat))\n","        parents = np.zeros(len(self.flat))\n","\n","        counter = 1\n","        for i in range(len(ids)):\n","            mentions = self.flat[i]\n","            if len(mentions) == 0:\n","                continue\n","            mentions.sort(key=lambda x: x.end_pos)\n","            for mention in mentions:\n","                # mention = self.clean_inconsistency(mentions, tree)\n","                end_pos = mention.end_pos + 1\n","                ids[mention.start_pos: end_pos] = counter\n","                parents[mention.start_pos: end_pos] = tree.get_parent_id(mention)\n","                mention.seq_id = counter\n","                counter += 1\n","        return ids, parents\n","\n","    def clean_inconsistency(self, mentions, tree):\n","        survivor = random.choice(mentions)\n","        for m in mentions:\n","            if m == survivor:\n","                continue\n","            tree.remove_mention(m)\n","        return survivor\n","\n","    def get_random_tree(self):\n","        tree = Tree()\n","        for cluster in self.clusters.values():\n","            subtree, mapping = self.get_random_subtree(cluster)\n","            tree.root.add_node(subtree)\n","            tree.update_mapping(mapping)\n","        return tree\n","\n","    @staticmethod\n","    def get_random_subtree(cluster):\n","        mapping = {}\n","        root = TreeNode(cluster.first_mention)\n","        mapping[cluster.first_mention] = root\n","\n","        max_height = 0\n","        for mention in cluster.mentions:\n","            next_root = root\n","            height = 1\n","            last = round(random.random() * max_height)\n","            for i in range(last):\n","                # only descent in tree in children that occur before the current mention\n","                possible_children = [x for x in next_root.children if x.mention.compare(mention) < 0]\n","\n","                if len(possible_children) == 0:\n","                    break\n","                next_root = random.choice(possible_children)\n","                height += 1\n","            node = next_root.add_mention(mention)\n","            mapping[mention] = node\n","\n","            if height >= max_height:\n","                max_height = height\n","\n","        return root, mapping\n","\n","\n","def build_structure(tokenizer, lines):\n","    structure = SuperStructure(tokenizer, lines)\n","\n","    for mention in tqdm(build_mention_list(lines)):\n","        structure.add_mention(mention)\n","\n","    return structure\n","\n","\n","def build_mention_list(lines):\n","    return build_mention_list_cort(lines)\n","\n","\n","def build_mention_list_boilerplate(lines):\n","    return mentions.build_mention_list(lines)\n","\n","\n","def build_mention_list_cort(lines):\n","    from cort.core import corpora\n","    from cort.core import mention_extractor\n","\n","    document_as_strings = []\n","\n","    current_document = \"\"\n","\n","    for line in lines:\n","        if line.startswith(\"#begin\") and current_document != \"\":\n","            document_as_strings.append(current_document)\n","            current_document = \"\"\n","        current_document += line\n","\n","    document_as_strings.append(current_document)\n","\n","    training_corpus = corpora.Corpus(\"test\", sorted([corpora.from_string(doc) for\n","                                                     doc in document_as_strings]))\n","    for idx, doc in enumerate(training_corpus):\n","        for mention in mention_extractor.extract_system_mentions(doc):\n","            # print(f\"New Mention:{mention}\")\n","\n","            if mention.span is None:\n","                continue\n","\n","            words = mention.attributes[\"tokens\"]\n","            start_pos = mention.span.begin\n","            end_pos = mention.span.end\n","            my_mention = Mention(f\"{idx}_A\", words, start_pos, end_pos)\n","            yield my_mention\n","\n","\n","def to_vectors(file_path, num_examples, tokenizer):\n","    lines = train_file_to_list(file_path)\n","    structure = build_structure(tokenizer, lines)\n","\n","    X1 = []\n","    X2 = []\n","    Y = []\n","\n","    if len(structure) == 0:\n","        return X1, X2, Y\n","\n","    found = set()\n","    for i in tqdm(range(num_examples * 3)):\n","        x1, x2, y = structure.get_random_example()\n","        ex_id = sum([sum(k) for k in y])\n","        if ex_id in found:\n","            continue\n","        found.add(ex_id)\n","        X1 += x1\n","        X2 += x2\n","        Y += y\n","        if len(found) >= num_examples:\n","            break\n","\n","    return X1, X2, Y\n","\n","\n","def create_tokenizer_from_hub_module():\n","    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","    with tf.Graph().as_default():\n","        bert_module = hub.Module(BERT_MODEL_HUB)\n","        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","        with tf.Session() as sess:\n","            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                                  tokenization_info[\"do_lower_case\"]])\n","\n","    return bert.tokenization.FullTokenizer(\n","        vocab_file=vocab_file, do_lower_case=do_lower_case)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1Mh6FEVZcpU","colab_type":"text"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"5SY3vGDJ9zpO","colab_type":"code","colab":{}},"source":["if __name__ == \"X __main__\":\n","    num_examples = 100\n","    tokenizer = create_tokenizer_from_hub_module()\n","\n","    with tqdm(os.walk(path_in), desc=\"folders\")as pb:\n","        for r, d, f in pb:\n","            pb.set_description(\"folder:...{}\".format(r[-20:]))\n","            for file_name in tqdm(f, desc=\"files\"):\n","                if not file_name.endswith(\"_conll\"):\n","                    continue\n","                x1, x2, y = to_vectors(os.path.join(r, file_name), num_examples, tokenizer)\n","\n","                _save_to_file(x1, path_out, file_name + \"_x1\")\n","                _save_to_file(x2, path_out, file_name + \"_x2\")\n","                _save_to_file(y, path_out, file_name + \"_y\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-y2CpYTB2V0O","colab_type":"text"},"source":["#Predicting"]},{"cell_type":"markdown","metadata":{"id":"Uu90EK-wuP_d","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"LezJdDcDJWoi","colab_type":"code","outputId":"26ca6a9b-1ca7-460e-9696-48f860945c63","executionInfo":{"status":"ok","timestamp":1565292762196,"user_tz":180,"elapsed":1006,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import tensorflow as tf\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","\n","from datetime import datetime\n","import tensorflow_hub as hub\n","import shutil\n","import sys\n","import pickle\n","import tf_metrics\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["W0808 19:32:41.460382 140495448348544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"eEYARr7GuRaL","colab_type":"text"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"sdaMPFg9JbBW","colab_type":"code","colab":{}},"source":["hooks = []\n","debug = {}\n","\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","OUTPUT_DIR = model_path\n","\n","\n","# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n","# Compute train and warmup steps from batch size\n","BATCH_SIZE = 4\n","MAX_EXAMPLES = 2000\n","LEARNING_RATE = 2e-5\n","# Warmup is a period of time where hte learning rate\n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","\n","MAX_SEQ_LEN = 512\n","num_classes = 768\n","pos_indices = list(range(1, num_classes))\n","average = 'micro'\n","features_path = sys.argv[-1]\n","MAX_CLASSES = 120\n","NON_ROOT_WEIGHT = 3\n","RANDOM_START = int(random.uniform(0,5))\n","min_loss = 1000\n","\n","tf.logging.set_verbosity(tf.logging.INFO)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7K2vNNsEvafP","colab_type":"text"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"UVoiG0YtJe-v","colab_type":"code","colab":{}},"source":["def build_estimator(labels):\n","    # Compute # train and warmup steps from batch size\n","    total_samples = sum([len(x) for x in labels])\n","    num_train_steps = int(total_samples / BATCH_SIZE)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","        num_labels=0,\n","        learning_rate=LEARNING_RATE,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","        model_fn=model_fn,\n","        config=run_config,\n","        params={\"batch_size\": BATCH_SIZE})\n","\n","    return estimator\n","\n","\n","def load_features(path, max_size=10000):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    if os.path.isfile(\"all_features.dmp\"):\n","        with open(\"all_features.dmp\", \"rb\")as f:\n","            labels, tokens, ids = pickle.load(f)\n","    else:\n","        for file_name in tqdm(sorted(list(os.listdir(path)))):\n","            df = pd.read_csv(f\"{path}/{file_name}\", header=None, names=list(range(MAX_SEQ_LEN)), index_col=None)\n","\n","            if file_name.endswith(\"y\"):\n","                # Changing -1 and 0\n","                df = df.apply(lambda row: [-(x + 1) if x <= 0 else x for x in row])\n","                # Moving all 1 to get all positive\n","                labels.append(df + 1)\n","            elif file_name.endswith(\"x1\"):\n","                tokens.append(df)\n","            elif file_name.endswith(\"x2\"):\n","                ids.append(df)\n","\n","            if len(tokens) >= max_size and \\\n","                    len(labels) >= max_size and \\\n","                    len(ids) >= max_size:\n","                break\n","\n","        with open(\"all_features.dmp\", \"wb\") as f:\n","            pickle.dump((labels, tokens, ids), f)\n","\n","    masks = [k.sum(axis=1).values > 0 for k in ids]\n","    for i in range(len(masks)):\n","        labels[i] = labels[i][masks[i]]\n","        tokens[i] = tokens[i][masks[i]]\n","        ids[i] = ids[i][masks[i]]\n","\n","    labels = [labels[i] for i in range(len(masks)) if len(labels[i]) > 0]\n","    tokens = [tokens[i] for i in range(len(masks)) if len(tokens[i]) > 0]\n","    ids = [ids[i] for i in range(len(masks)) if len(ids[i]) > 0]\n","\n","    return tokens, ids, labels\n","\n","\n","def slice_input(tokens, ids, labels):\n","    print(\"Slicing\")\n","\n","    def fix_it(df):\n","        # print(f\"list values len {len(df)}\")\n","        # print(f\"shape 0 {df[0].shape}\")\n","        df = pd.concat(df)\n","        # print(f\"New df:{df.shape}\")\n","        return df.fillna(0).values.astype('int32')\n","\n","    sliced_labels = fix_it(labels)\n","    sliced_tokens = fix_it(tokens)\n","    sliced_ids = fix_it(ids)\n","    sliced_mask = (sliced_ids > 0).astype('int32')\n","\n","    print(\"Sliced\")\n","    return input_fn_builder(sliced_tokens, sliced_ids, sliced_labels, sliced_mask, False, False, sliced_mask)\n","\n","\n","def input_fn_builder(tokens, ids, labels, mask, is_training, drop_remainder, loss_weight):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    # print(f\"Tokens shape:{tokens.shape}\")\n","\n","    num_examples, seq_length = tokens.shape\n","\n","    # print(f\"t {num_examples}, {seq_length}\")\n","\n","    all_input_ids = tokens\n","    all_input_mask = mask\n","    all_segment_ids = ids\n","    all_label_ids = labels\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # This is for demo purposes and does NOT scale to large data sets. We do\n","        # not use Dataset.from_generator() because that uses tf.py_func which is\n","        # not TPU compatible. The right way to load data is with TFRecordReader.\n","        d = tf.data.Dataset.from_tensor_slices({\n","            \"input_ids\": all_input_ids,\n","            \"input_mask\": all_input_mask,\n","            \"segment_ids\": all_segment_ids,\n","            \"label_ids\": all_label_ids,\n","            \"loss_weight\": loss_weight\n","        })\n","\n","        if is_training:\n","            # d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","        d = d.prefetch(batch_size)\n","        return d\n","\n","    return input_fn\n","\n","\n","def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, loss_weight):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    bert_module = hub.Module(\n","        BERT_MODEL_HUB,\n","        trainable=True)\n","    bert_inputs = dict(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids)\n","    bert_outputs = bert_module(\n","        inputs=bert_inputs,\n","        signature=\"tokens\",\n","        as_dict=True)\n","\n","    one_hot_labels = tf.one_hot(labels, MAX_CLASSES, name=\"My_OneHot\")\n","\n","    # Use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"sequence_output\"]\n","    hidden_size = output_layer.shape[-1].value\n","\n","    output_weights = tf.get_variable(\n","        \"output_weights\", [MAX_CLASSES, hidden_size], \n","        initializer=tf.truncated_normal_initializer(mean=0.0, stddev=1))\n","\n","    output_bias = tf.get_variable(\n","        \"output_bias\", [MAX_SEQ_LEN, MAX_CLASSES], initializer=tf.zeros_initializer())\n","\n","    with tf.variable_scope(\"loss\"):\n","        output_layer_2 = tf.matmul(output_layer, output_weights, transpose_b=True)\n","        output_layer_2 = tf.add(output_layer_2, output_bias)\n","        #output_layer_2 = tf.layers.batch_normalization(output_layer_2, training=(not is_predicting))\n","        #output_layer_2 = tf.math.sigmoid(output_layer_2)\n","        output_layer_2 = tf.nn.relu(output_layer_2)\n","        \n","        # Dropout helps prevent overfitting\n","        # output_layer = tf.nn.dropout(output_layer, keep_prob=0.9, name=\"My_Dropout\")\n","        # print(f\"1 output_layer shape:{output_layer.shape}\")\n","\n","        # output_layer = tf.nn.softmax(output_layer, name=\"My Softmax\")\n","        # print(f\"2 output_layer shape:{output_layer.shape}\")\n","\n","        # tmp_mask = tf.broadcast_to(tf.expand_dims(input_mask, axis=-1), tf.shape(output_layer))\n","        # output_layer = tf.math.multiply(output_layer, tf.cast(tmp_mask, tf.float32), name=\"My_Argmax_Mask\")\n","\n","        # softmax = tf.nn.softmax(output_layer_2, name=\"My_Softmax\")\n","        argmax = tf.math.argmax(output_layer_2, axis=-1, name=\"My_Argmax\")\n","\n","        predicted_labels = tf.math.multiply(argmax, tf.cast(input_mask, tf.int64), name=\"My_Argmax_Mask\")\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                # \"softmax\": softmax[0][0],\n","                # \"label\": labels[0],\n","                # \"argmax\": argmax[0][0],\n","                # \"segment_ids\": segment_ids[0],\n","                # \"output_layer\": output_layer[0][0],\n","                \"predicted\": predicted_labels[0],\n","                \"output_layer_2\": output_layer_2[0][0],\n","                \"output_weights\": output_weights[0],\n","                \"output_bias\": output_bias[0],\n","                # \"OK labels\": tf.math.reduce_sum(\n","                #    tf.to_float(tf.math.equal(tf.cast(labels, tf.int64), predicted_labels))),\n","                # \"Total Labels\": tf.shape(labels),\n","                # \"softmax shape\": tf.shape(softmax),\n","                # \"argmax shape\": tf.shape(argmax),\n","                # \"predicted_labels shape\": tf.shape(predicted_labels),\n","                # \"input_mask shape\": tf.shape(input_mask)\n","            }, every_n_iter=10))\n","\n","        if is_predicting:\n","            return predicted_labels\n","\n","        # output_layer = tf.reshape(output_layer, [BATCH_SIZE ,MAX_SEQ_LEN, 768])\n","        # print(f\"logits : {output_layer.shape}\")\n","        # print(f\"tmp : {tmp.shape}\")\n","\n","        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=output_layer_2, name=\"My_Loss\")\n","        no_loss = tf.multiply(loss, tf.cast(input_mask, tf.float32))\n","        weightened = tf.divide(no_loss, tf.cast(tf.reduce_sum(input_mask), tf.float32))\n","        rm = tf.reduce_sum(weightened)\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                # \"count no_loss\": tf.math.count_nonzero(loss),\n","                # \"count input_mask\": tf.math.count_nonzero(input_mask),\n","                # \"loss shape\": tf.shape(loss),\n","                # \"rm shape\": tf.shape(rm),\n","                # \"no_loss shape\": tf.shape(no_loss),\n","                # \"one_hot_labels shape\": tf.shape(one_hot_labels),\n","                \"no_loss\": no_loss[0],\n","                # \"weightened\": weightened[0],\n","                \"loss_weight\": loss_weight[0],\n","                \"final_loss\": rm\n","            }, every_n_iter=10))\n","\n","        return rm, predicted_labels\n","\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","        loss_weight = features[\"loss_weight\"]\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","\n","            (loss, predicted_labels) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, loss_weight)\n","            \n","            hooks.clear()\n","            hooks.append(tf.train.LoggingTensorHook({\"loss\": loss}, every_n_iter=10))\n","\n","            train_op = bert.optimization.create_optimizer(\n","                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            # Calculate evaluation metrics.\n","            def metric_fn(label_ids, predicted_labels):\n","                y_true, y_pred = label_ids, predicted_labels\n","\n","                precision = tf_metrics.precision(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                recall = tf_metrics.recall(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                f2 = tf_metrics.fbeta(\n","                    y_true, y_pred, num_classes, pos_indices, average=average, beta=2)\n","                f1 = tf_metrics.f1(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","\n","                return {\n","                    \"precision\": precision,\n","                    \"recall\": recall,\n","                    \"f2\": f2,\n","                    \"f1\": f1\n","                }\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  train_op=train_op,\n","                                                  training_hooks=hooks)\n","            else:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  eval_metric_ops=eval_metrics)\n","        else:\n","            (predicted_labels, log_probs) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            predictions = {\n","                'probabilities': log_probs,\n","                'labels': predicted_labels\n","            }\n","            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","    # Return the actual model function in the closure\n","    return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQMMjNrwvfKW","colab_type":"text"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"bIFhze2f0NgJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"87466693-9890-497d-bb4e-8dd60769fa21","executionInfo":{"status":"ok","timestamp":1565294489074,"user_tz":180,"elapsed":2698,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}}},"source":["tokens, ids, labels = load_features(path_out, MAX_EXAMPLES)\n","estimator = build_estimator(labels)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["I0808 20:01:28.317853 140495448348544 estimator.py:209] Using config: {'_model_dir': '/content/gdrive/My Drive/Puc/Projeto Final/Datasets/../models/bert/trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc6aa0b6ba8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tTowatZbwTwR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"3f54aa48-bcb9-4048-bbd6-889c082eb5a5","executionInfo":{"status":"error","timestamp":1565296583381,"user_tz":180,"elapsed":5396,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}}},"source":["with tf.compat.v1.Session() as sess:\n","  predict_input_fn = slice_input(tokens, ids, labels)\n","  predictions = estimator.predict(predict_input_fn)\n","\n","  p = sess.run(predictions)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["Slicing\n","Sliced\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-6dab1b7e9df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1096\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1099\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."]}]},{"cell_type":"code","metadata":{"id":"f7E1Xba92a0h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"f5cec0f5-0e86-4f36-d1f8-aff0798823e1","executionInfo":{"status":"error","timestamp":1565295036163,"user_tz":180,"elapsed":6761,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}}},"source":["p = [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(tokens, predictions)]"],"execution_count":45,"outputs":[{"output_type":"stream","text":["I0808 20:10:31.857362 140495448348544 estimator.py:1145] Calling model_fn.\n","I0808 20:10:35.113906 140495448348544 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-d643aebed22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-45-d643aebed22b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    617\u001b[0m             input_fn, ModeKeys.PREDICT)\n\u001b[1;32m    618\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 619\u001b[0;31m             features, None, ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-4085bd2ad492>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             (predicted_labels, log_probs) = create_model(\n\u001b[0;32m--> 315\u001b[0;31m                 is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             predictions = {\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       raise TypeError(\n\u001b[0;32m--> 477\u001b[0;31m           \u001b[0;34m\"Tensor objects are only iterable when eager execution is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m           \"enabled. To iterate over this tensor use tf.map_fn.\")\n\u001b[1;32m    479\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn."]}]},{"cell_type":"code","metadata":{"id":"BT2TQBBe_PO2","colab_type":"code","outputId":"11abb91a-7cf9-4583-8e06-cd2f5b042d68","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["\n","current_time = datetime.now()\n","\n","\n","NUM_TRAIN_EPOCHS = int(max([len(x) for x in labels])/5)\n","print(f'Beginning Training! {NUM_TRAIN_EPOCHS}')\n","\n","\n","\n","seq_epochs = list(range(NUM_TRAIN_EPOCHS))\n","random.shuffle(seq_epochs)\n","\n","for i,epoch in enumerate(seq_epochs):\n","    print(f\"New Epoch {i}\")\n","    train_input_fn = slice_input(epoch, tokens, ids, labels, True)\n","    eval_input_fn = slice_input(epoch, tokens, ids, labels, False)\n","\n","    #print(f\"hooks:{len(hooks)}\")\n","    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n","    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n","    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","\n","    # estimator.train(input_fn=slice_input(epoch, tokens, ids, labels, True), hooks=hooks)\n","    hooks.clear()\n","\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["I0805 01:59:37.180501 139886290950016 estimator.py:1147] Done calling model_fn.\n","I0805 01:59:37.187224 139886290950016 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n","I0805 01:59:38.981978 139886290950016 monitored_session.py:240] Graph was finalized.\n","I0805 01:59:38.994852 139886290950016 saver.py:1280] Restoring parameters from /content/gdrive/My Drive/Puc/Projeto Final/Datasets/bert/trained/model.ckpt-7874\n","I0805 01:59:44.370744 139886290950016 session_manager.py:500] Running local_init_op.\n","I0805 01:59:44.728851 139886290950016 session_manager.py:502] Done running local_init_op.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JrGj4ccIfAj9","colab_type":"code","colab":{}},"source":["#FINAL_DEST = f\"{root}/trained2\"\n","#shutil.rmtree(FINAL_DEST, ignore_errors=True)\n","#shutil.copytree(OUTPUT_DIR, FINAL_DEST)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2ztkHXANV_F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
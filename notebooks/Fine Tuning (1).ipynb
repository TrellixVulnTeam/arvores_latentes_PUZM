{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine Tuning","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MdnkFOHt6oHm","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BKsmYkN6pjJ","colab_type":"code","colab":{}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXrE0lmb67Sr","colab_type":"code","colab":{}},"source":["%tensorboard --logdir /content/gdrive/My\\ Drive/Puc/Projeto\\ Final/models/bert/E4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwEqRS-j7L7P","colab_type":"code","colab":{}},"source":["cp -R BERT /content/gdrive/My\\ Drive/Puc/Projeto\\ Final/models/bert/M1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yqQyhsoaJqFe","colab_type":"text"},"source":["# Header"]},{"cell_type":"markdown","metadata":{"id":"fMKHndOWJsUK","colab_type":"text"},"source":["## Install"]},{"cell_type":"code","metadata":{"id":"n49bClA5Im33","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","!pip install bert-tensorflow\n","!pip install git+https://github.com/guillaumegenthial/tf_metrics.git\n","  \n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ONgBtOlJxpJ","colab_type":"text"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"JEPsT9Tf2SON","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","\n","from datetime import datetime\n","import tensorflow_hub as hub\n","import shutil\n","import sys\n","import pickle\n","import tf_metrics\n","import random\n","import os\n","from bert.optimization import AdamWeightDecayOptimizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CT0gXiJnJy45","colab_type":"text"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"sdaMPFg9JbBW","colab_type":"code","colab":{}},"source":["hooks = []\n","debug = {}\n","\n","root = r\"/content/gdrive/My Drive/Puc/Projeto Final\"\n","data_folder = f\"{root}/Datasets/finetuning/train/\"\n","OUTPUT_DIR = \"BERT\" # f\"{root}/models/bert/multi_cased_devel\"\n","\n","# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n","# Compute train and warmup steps from batch size\n","BATCH_SIZE = 4\n","MAX_EXAMPLES = 20000\n","LEARNING_RATE = 2e-5\n","# Warmup is a period of time where hte learning rate\n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","MAX_SEQ_LEN = 512\n","num_classes = 768\n","pos_indices = list(range(1, num_classes))\n","average = 'micro'\n","features_path = sys.argv[-1]\n","MAX_CLASSES = 120\n","NON_ROOT_WEIGHT = 3\n","min_loss = 1000\n","SPLIT_SIZE = 9\n","RANDOM_START = int(random.uniform(0, SPLIT_SIZE))\n","\n","tf.logging.set_verbosity(tf.logging.INFO)\n","#shutil.rmtree(OUTPUT_DIR, ignore_errors=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vs0FCzwDJ1Gh","colab_type":"text"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"UVoiG0YtJe-v","colab_type":"code","colab":{}},"source":["def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n","  \"\"\"Creates an optimizer training op.\"\"\"\n","  global_step = tf.train.get_or_create_global_step()\n","\n","  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n","\n","  # Implements linear decay of the learning rate.\n","  learning_rate = tf.train.polynomial_decay(\n","      learning_rate,\n","      global_step,\n","      num_train_steps,\n","      end_learning_rate=0.0,\n","      power=1.0,\n","      cycle=False)\n","\n","  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n","  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n","  if num_warmup_steps:\n","    global_steps_int = tf.cast(global_step, tf.int32)\n","    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n","\n","    global_steps_float = tf.cast(global_steps_int, tf.float32)\n","    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n","\n","    warmup_percent_done = global_steps_float / warmup_steps_float\n","    warmup_learning_rate = init_lr * warmup_percent_done\n","\n","    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n","    learning_rate = (\n","        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n","\n","  # It is recommended that you use this optimizer for fine tuning, since this\n","  # is how the model was trained (note that the Adam m/v variables are NOT\n","  # loaded from init_checkpoint.)\n","  optimizer = AdamWeightDecayOptimizer(\n","      learning_rate=learning_rate,\n","      weight_decay_rate=0.01,\n","      beta_1=0.9,\n","      beta_2=0.999,\n","      epsilon=1e-6,\n","      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n","\n","  if use_tpu:\n","    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n","\n","  tvars = tf.trainable_variables()\n","  grads = tf.gradients(loss, tvars)\n","\n","  # This is how the model was pre-trained.\n","  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n","\n","  train_op = optimizer.apply_gradients(\n","      zip(grads, tvars), global_step=global_step)\n","\n","  # Normally the global step update is done inside of `apply_gradients`.\n","  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n","  # a different optimizer, you should probably take this line out.\n","  new_global_step = global_step + 1\n","  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n","  return train_op, optimizer\n","\n","def build_estimator(labels):\n","    # Compute # train and warmup steps from batch size\n","    total_samples = sum([len(x) for x in labels])\n","    num_train_steps = int(total_samples * NUM_TRAIN_EPOCHS / BATCH_SIZE)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","        num_labels=0,\n","        learning_rate=LEARNING_RATE,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","        model_fn=model_fn,\n","        config=run_config,\n","        params={\"batch_size\": BATCH_SIZE})\n","\n","    return estimator\n","\n","\n","def load_features(path, max_size=20000):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    if os.path.isfile(\"all_features.dmp\"):\n","        with open(\"all_features.dmp\", \"rb\")as f:\n","            labels, tokens, ids = pickle.load(f)\n","    else:\n","        for file_name in tqdm(sorted(list(os.listdir(path)))):\n","            df = pd.read_csv(f\"{path}/{file_name}\", header=None, names=list(range(MAX_SEQ_LEN)), index_col=None)\n","\n","            if file_name.endswith(\"y\"):\n","                # Changing -1 and 0\n","                df = df.apply(lambda row: [-(x + 1) if x <= 0 else x for x in row])\n","                # Moving all 1 to get all positive\n","                labels.append(df + 1)\n","            elif file_name.endswith(\"x1\"):\n","                tokens.append(df)\n","            elif file_name.endswith(\"x2\"):\n","                ids.append(df)\n","\n","            if len(tokens) >= max_size and \\\n","                    len(labels) >= max_size and \\\n","                    len(ids) >= max_size:\n","                break\n","\n","        with open(\"all_features.dmp\", \"wb\") as f:\n","            pickle.dump((labels, tokens, ids), f)\n","\n","    masks = [k.sum(axis=1).values > 0 for k in ids]\n","    for i in range(len(masks)):\n","        labels[i] = labels[i][masks[i]]\n","        tokens[i] = tokens[i][masks[i]]\n","        ids[i] = ids[i][masks[i]]\n","\n","    labels = [labels[i] for i in range(len(masks)) if len(labels[i]) > 0]\n","    tokens = [tokens[i] for i in range(len(masks)) if len(tokens[i]) > 0]\n","    ids = [ids[i] for i in range(len(masks)) if len(ids[i]) > 0]\n","\n","    return tokens, ids, labels\n","\n","\n","def slice_input(epoch, o_tokens, o_ids, o_labels, train=True):\n","    print(f\"Slicing {epoch + RANDOM_START}\")\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    def slice(df):\n","        full = len(df) / SPLIT_SIZE\n","        chunk = int((epoch + RANDOM_START) % full)\n","        # Doing a 80-20 train-test split\n","        slice_start = chunk * SPLIT_SIZE\n","        slice_end = slice_start + (SPLIT_SIZE - 1)\n","\n","        if not train:\n","            slice_start = slice_end\n","            slice_end = slice_start + 1\n","\n","        return df.iloc[slice_start:slice_end, :]\n","\n","    for i in range(len(o_tokens)):\n","        labels.append(slice(o_labels[i]))\n","        tokens.append(slice(o_tokens[i]))\n","        ids.append(slice(o_ids[i]))\n","\n","    def fix_it(df):\n","        # print(f\"list values len {len(df)}\")\n","        # print(f\"shape 0 {df[0].shape}\")\n","        df = pd.concat(df)\n","        # print(f\"New df:{df.shape}\")\n","        return df.fillna(0).values.astype('int32')\n","\n","    sliced_labels = fix_it(labels)\n","    sliced_tokens = fix_it(tokens)\n","    sliced_ids = fix_it(ids)\n","    sliced_mask = (sliced_ids > 0).astype('int32')\n","\n","    sliced_loss_weight = (sliced_ids > 1) * (NON_ROOT_WEIGHT - 1)  # Will add 1 in row above\n","    sliced_loss_weight += sliced_mask\n","\n","    print(\"Sliced\")\n","    return input_fn_builder(sliced_tokens, sliced_ids, sliced_labels, sliced_mask, train, False, sliced_loss_weight)\n","\n","\n","def input_fn_builder(tokens, ids, labels, mask, is_training, drop_remainder, loss_weight):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    # print(f\"Tokens shape:{tokens.shape}\")\n","\n","    num_examples, seq_length = tokens.shape\n","\n","    # print(f\"t {num_examples}, {seq_length}\")\n","\n","    all_input_ids = tokens\n","    all_input_mask = mask\n","    all_segment_ids = ids\n","    all_label_ids = labels\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # This is for demo purposes and does NOT scale to large data sets. We do\n","        # not use Dataset.from_generator() because that uses tf.py_func which is\n","        # not TPU compatible. The right way to load data is with TFRecordReader.\n","        d = tf.data.Dataset.from_tensor_slices({\n","            \"input_ids\": all_input_ids,\n","            \"input_mask\": all_input_mask,\n","            \"segment_ids\": all_segment_ids,\n","            \"label_ids\": all_label_ids,\n","            \"loss_weight\": loss_weight\n","        })\n","\n","        if is_training:\n","            # d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","        d = d.prefetch(batch_size)\n","        return d\n","\n","    return input_fn\n","\n","\n","def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, loss_weight):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    bert_module = hub.Module(\n","        BERT_MODEL_HUB,\n","        trainable=True)\n","    bert_inputs = dict(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids)\n","    bert_outputs = bert_module(\n","        inputs=bert_inputs,\n","        signature=\"tokens\",\n","        as_dict=True)\n","\n","    one_hot_labels = tf.one_hot(labels, MAX_CLASSES, name=\"My_OneHot\")\n","\n","    # Use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"sequence_output\"]\n","    hidden_size = output_layer.shape[-1].value\n","\n","    output_weights = tf.get_variable(\n","        \"output_weights\", [MAX_CLASSES, hidden_size], \n","        initializer=tf.truncated_normal_initializer(mean=0.0, stddev=1))\n","\n","    output_bias = tf.get_variable(\n","        \"output_bias\", [MAX_SEQ_LEN, MAX_CLASSES], initializer=tf.zeros_initializer())\n","\n","    with tf.variable_scope(\"loss\"):\n","        output_layer_2 = tf.matmul(output_layer, output_weights, transpose_b=True)\n","        output_layer_2 = tf.add(output_layer_2, output_bias)\n","        # output_layer_2 = tf.layers.batch_normalization(output_layer_2, training=(not is_predicting))\n","        # output_layer_2 = tf.math.sigmoid(output_layer_2, name=\"My_output_layer\")\n","        output_layer_2 = tf.nn.relu(output_layer_2, name=\"My_output_layer\")\n","        \n","        # Dropout helps prevent overfitting\n","        # output_layer = tf.nn.dropout(output_layer, keep_prob=0.9, name=\"My_Dropout\")\n","        # print(f\"1 output_layer shape:{output_layer.shape}\")\n","\n","        # output_layer = tf.nn.softmax(output_layer, name=\"My Softmax\")\n","        # print(f\"2 output_layer shape:{output_layer.shape}\")\n","\n","        # tmp_mask = tf.broadcast_to(tf.expand_dims(input_mask, axis=-1), tf.shape(output_layer))\n","        # output_layer = tf.math.multiply(output_layer, tf.cast(tmp_mask, tf.float32), name=\"My_Argmax_Mask\")\n","\n","        # softmax = tf.nn.softmax(output_layer_2, name=\"My_Softmax\")\n","        argmax = tf.math.argmax(output_layer_2, axis=-1, name=\"My_Argmax\")\n","\n","        predicted_labels = tf.math.multiply(argmax, tf.cast(input_mask, tf.int64), name=\"d\")\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                # \"softmax\": softmax[0][0],\n","                # \"label\": labels[0],\n","                # \"argmax\": argmax[0][0],\n","                # \"segment_ids\": segment_ids[0],\n","                # \"output_layer\": output_layer[0][0],\n","                \"predicted\": predicted_labels[0],\n","                \"output_layer_2\": output_layer_2[0][0],\n","                \"output_weights\": output_weights[0],\n","                \"output_bias\": output_bias[0],\n","                # \"OK labels\": tf.math.reduce_sum(\n","                #    tf.to_float(tf.math.equal(tf.cast(labels, tf.int64), predicted_labels))),\n","                # \"Total Labels\": tf.shape(labels),\n","                # \"softmax shape\": tf.shape(softmax),\n","                # \"argmax shape\": tf.shape(argmax),\n","                # \"predicted_labels shape\": tf.shape(predicted_labels),\n","                # \"input_mask shape\": tf.shape(input_mask)\n","            }, every_n_iter=10))\n","\n","        if is_predicting:\n","            return predicted_labels\n","\n","        # output_layer = tf.reshape(output_layer, [BATCH_SIZE ,MAX_SEQ_LEN, 768])\n","        # print(f\"logits : {output_layer.shape}\")\n","        # print(f\"tmp : {tmp.shape}\")\n","\n","        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=output_layer_2, name=\"My_softmax_loss\")\n","        no_loss = tf.multiply(loss, tf.cast(input_mask, tf.float32), name=\"My_masked_loss\")\n","        weightened = tf.divide(no_loss, tf.cast(tf.reduce_sum(input_mask), tf.float32), name=\"My_wloss\")\n","        rm = tf.reduce_sum(weightened, name=\"My_final_loss\")\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                # \"count no_loss\": tf.math.count_nonzero(loss),\n","                # \"count input_mask\": tf.math.count_nonzero(input_mask),\n","                # \"loss shape\": tf.shape(loss),\n","                # \"rm shape\": tf.shape(rm),\n","                # \"no_loss shape\": tf.shape(no_loss),\n","                # \"one_hot_labels shape\": tf.shape(one_hot_labels),\n","                \"no_loss\": no_loss[0],\n","                # \"weightened\": weightened[0],\n","                \"loss_weight\": loss_weight[0],\n","                \"final_loss\": rm\n","            }, every_n_iter=10))\n","\n","        return rm, predicted_labels\n","\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","        loss_weight = features[\"loss_weight\"]\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","\n","            (loss, predicted_labels) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, loss_weight)\n","            \n","            train_op, optimizer = create_optimizer(\n","                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            hooks.clear()\n","            hooks.append(tf.train.LoggingTensorHook({\"loss\": loss, \"lr\":optimizer.learning_rate}, every_n_iter=10))\n","\n","            # Calculate evaluation metrics.\n","            def metric_fn(label_ids, predicted_labels):\n","                y_true, y_pred = label_ids, predicted_labels\n","\n","                precision = tf_metrics.precision(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                recall = tf_metrics.recall(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                f2 = tf_metrics.fbeta(\n","                    y_true, y_pred, num_classes, pos_indices, average=average, beta=2)\n","                f1 = tf_metrics.f1(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","\n","                return {\n","                    \"precision\": precision,\n","                    \"recall\": recall,\n","                    \"f2\": f2,\n","                    \"f1\": f1\n","                }\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  train_op=train_op,\n","                                                  training_hooks=hooks)\n","            else:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  eval_metric_ops=eval_metrics)\n","        else:\n","            (predicted_labels, log_probs) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            predictions = {\n","                'probabilities': log_probs,\n","                'labels': predicted_labels\n","            }\n","            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","    # Return the actual model function in the closure\n","    return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AH0Wt_DpJ6Zh","colab_type":"text"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"BT2TQBBe_PO2","colab_type":"code","colab":{}},"source":["current_time = datetime.now()\n","\n","print(\"Loading Features\")\n","tokens, ids, labels = load_features(data_folder, MAX_EXAMPLES)\n","\n","NUM_TRAIN_EPOCHS = int(max([len(x) for x in labels])/SPLIT_SIZE)\n","print(f'Beginning Training! {NUM_TRAIN_EPOCHS}')\n","\n","estimator = build_estimator(labels)\n","\n","seq_epochs = list(range(NUM_TRAIN_EPOCHS))\n","random.shuffle(seq_epochs)\n","\n","for i,epoch in enumerate(seq_epochs):\n","    print(f\"New Epoch {i}\")\n","    train_input_fn = slice_input(epoch, tokens, ids, labels, True)\n","    eval_input_fn = slice_input(epoch, tokens, ids, labels, False)\n","\n","    #print(f\"hooks:{len(hooks)}\")\n","    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n","    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n","    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","\n","    # estimator.train(input_fn=slice_input(epoch, tokens, ids, labels, True), hooks=hooks)\n","    hooks.clear()\n","    \n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wF657XjXooqF","colab_type":"code","colab":{}},"source":["%tensorboard --logdir BERT"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"clVgpa8k9c3S","colab_type":"code","colab":{}},"source":["%tensorboard --logdir new:BERT,old:/content/gdrive/My\\ Drive/Puc/Projeto\\ Final/models/bert/train/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gb6VOQSoQ77","colab_type":"code","colab":{}},"source":["!cp -R BERT/ /content/gdrive/My\\ Drive/Puc/Projeto\\ Final/models/bert/train/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6X3j3UGfZPP","colab_type":"code","colab":{}},"source":["!cp -R BERT/* /content/gdrive/My\\ Drive/Puc/Projeto\\ Final/models/bert/train/"],"execution_count":0,"outputs":[]}]}
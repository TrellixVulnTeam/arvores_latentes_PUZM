{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine Tuning TPU","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"JEPsT9Tf2SON","colab_type":"code","outputId":"673afcb6-454f-4cb8-a9bf-3a368f63340c","executionInfo":{"status":"ok","timestamp":1564834810104,"user_tz":180,"elapsed":23662,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","drive.mount('/content/gdrive/')\n","\n","root = r\"/content/gdrive/My Drive/Puc/Projeto Final/Datasets/bert\"\n","data_folder = \"train/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tzo7TJwKQ30y","colab_type":"text"},"source":["!pip install git+https://github.com/guillaumegenthial/tf_metrics.git\n","!pip install tensorflow-gpu --force-reinstall"]},{"cell_type":"code","metadata":{"id":"9W4yFaEZNkls","colab_type":"code","outputId":"14cb3e6b-55af-456d-8856-a8b7ccf61823","executionInfo":{"status":"ok","timestamp":1564834941712,"user_tz":180,"elapsed":98389,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":680}},"source":["!pip install bert-tensorflow\n","!pip install git+https://github.com/guillaumegenthial/tf_metrics.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Collecting git+https://github.com/guillaumegenthial/tf_metrics.git\n","  Cloning https://github.com/guillaumegenthial/tf_metrics.git to /tmp/pip-req-build-5o2lmrtn\n","  Running command git clone -q https://github.com/guillaumegenthial/tf_metrics.git /tmp/pip-req-build-5o2lmrtn\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.16.4)\n","Collecting tensorflow-gpu>=1.6 (from tf-metrics==0.0.1)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n","\u001b[K     |████████████████████████████████| 377.0MB 70kB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.11.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.0.8)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.7.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.1.7)\n","Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.15.0)\n","Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.33.4)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.15.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.1.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (41.0.1)\n","Building wheels for collected packages: tf-metrics\n","  Building wheel for tf-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tf-metrics: filename=tf_metrics-0.0.1-cp36-none-any.whl size=7694 sha256=f68c26b5ac7f582808904213157b7b52c1f9c192755e20903995753627231ad3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-53cthhxi/wheels/da/6c/c8/663ef339a0666590dc53bd13bab86643a1f9c35b26742d7876\n","Successfully built tf-metrics\n","Installing collected packages: tensorflow-gpu, tf-metrics\n","Successfully installed tensorflow-gpu-1.14.0 tf-metrics-0.0.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"LezJdDcDJWoi","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","\n","from datetime import datetime\n","import tensorflow_hub as hub\n","import shutil\n","import sys\n","import pickle\n","import tf_metrics\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdaMPFg9JbBW","colab_type":"code","colab":{}},"source":["hooks = []\n","debug = {}\n","\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","# Compute train and warmup steps from batch size\n","BATCH_SIZE = 8\n","MAX_EXAMPLES = 2000\n","LEARNING_RATE = 2e-5\n","# Warmup is a period of time where hte learning rate\n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","OUTPUT_DIR = \"BERT\"\n","MAX_SEQ_LEN = 512\n","num_classes = 768\n","pos_indices = list(range(1, num_classes))\n","average = 'micro'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVoiG0YtJe-v","colab_type":"code","colab":{}},"source":["def build_estimator(labels):\n","    # Compute # train and warmup steps from batch size\n","    total_samples = len(labels)\n","    num_train_steps = int(total_samples / BATCH_SIZE)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.tpu.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","        num_labels=0,\n","        learning_rate=LEARNING_RATE,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.contrib.tpu.TPUEstimator(\n","        model_fn=model_fn,\n","        config=run_config,\n","        params={\"batch_size\": BATCH_SIZE})\n","\n","    return estimator\n","\n","\n","def load_features(path, max_size=10000):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    if os.path.isfile(\"all_features.dmp\"):\n","        with open(\"all_features.dmp\", \"rb\")as f:\n","            labels, tokens, ids = pickle.load(f)\n","    else:\n","        for file_name in tqdm(sorted(list(os.listdir(path)))):\n","            df = pd.read_csv(f\"{path}/{file_name}\", header=None, names=list(range(MAX_SEQ_LEN)), index_col=None)\n","\n","            if file_name.endswith(\"y\"):\n","                # Changing -1 and 0\n","                df = df.apply(lambda row: [-(x + 1) if x <= 0 else x for x in row])\n","                # Moving all 1 to get all positive\n","                labels.append(df + 1)\n","            elif file_name.endswith(\"x1\"):\n","                tokens.append(df)\n","            elif file_name.endswith(\"x2\"):\n","                ids.append(df)\n","\n","            if len(tokens) >= max_size and \\\n","                    len(labels) >= max_size and \\\n","                    len(ids) >= max_size:\n","                break\n","\n","        with open(\"all_features.dmp\", \"wb\") as f:\n","            pickle.dump((labels, tokens, ids), f)\n","\n","    masks = [k.sum(axis=1).values >= 0 for k in ids]\n","    for i in range(len(masks)):\n","        labels[i] = labels[i][masks[i]]\n","        tokens[i] = tokens[i][masks[i]]\n","        ids[i] = ids[i][masks[i]]\n","\n","    labels = [labels[i] for i in range(len(masks)) if len(labels[i]) > 0]\n","    tokens = [tokens[i] for i in range(len(masks)) if len(tokens[i]) > 0]\n","    ids = [ids[i] for i in range(len(masks)) if len(ids[i]) > 0]\n","\n","    return tokens, ids, labels\n","\n","\n","def slice_input(epoch, o_tokens, o_ids, o_labels, train=True):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    def slice(df):\n","        full = len(df) / 5\n","        chunk = int(epoch % full)\n","        # Doing a 80-20 train-test split\n","        slice_start = chunk * 5\n","        slice_end = slice_start + 4\n","\n","        if not train:\n","            slice_start = slice_end\n","            slice_end = slice_start + 1\n","\n","        return df.iloc[slice_start:slice_end, :]\n","\n","    for i in range(len(o_tokens)):\n","        labels.append(slice(o_labels[i]))\n","        tokens.append(slice(o_tokens[i]))\n","        ids.append(slice(o_ids[i]))\n","\n","    def fix_it(df):\n","        print(f\"list values len {len(df)}\")\n","        print(f\"shape 0 {df[0].shape}\")\n","        df = pd.concat(df)\n","        print(f\"New df:{df.shape}\")\n","        return df.fillna(0).values.astype('int32')\n","\n","    sliced_labels = fix_it(labels)\n","    sliced_tokens = fix_it(tokens)\n","    sliced_ids = fix_it(ids)\n","    sliced_mask = (sliced_ids > 0).astype('int32')\n","\n","    return input_fn_builder(sliced_tokens, sliced_ids, sliced_labels, sliced_mask, train, False)\n","\n","\n","def input_fn_builder(tokens, ids, labels, mask, is_training, drop_remainder):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    print(f\"Tokens shape:{tokens.shape}\")\n","\n","    num_examples, seq_length = tokens.shape\n","\n","    print(f\"t {num_examples}, {seq_length}\")\n","\n","    all_input_ids = tokens\n","    all_input_mask = mask\n","    all_segment_ids = ids\n","    all_label_ids = labels\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # This is for demo purposes and does NOT scale to large data sets. We do\n","        # not use Dataset.from_generator() because that uses tf.py_func which is\n","        # not TPU compatible. The right way to load data is with TFRecordReader.\n","        d = tf.data.Dataset.from_tensor_slices({\n","            \"input_ids\": all_input_ids,\n","            \"input_mask\": all_input_mask,\n","            \"segment_ids\": all_segment_ids,\n","            \"label_ids\": all_label_ids\n","        })\n","\n","        if is_training:\n","            # d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","        d = d.prefetch(batch_size)\n","        return d\n","\n","    return input_fn\n","\n","\n","def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    bert_module = hub.Module(\n","        BERT_MODEL_HUB,\n","        trainable=True)\n","    bert_inputs = dict(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids)\n","    bert_outputs = bert_module(\n","        inputs=bert_inputs,\n","        signature=\"tokens\",\n","        as_dict=True)\n","\n","    print(f\"input_ids:{input_ids.shape}\")\n","    print(f\"input_mask:{input_mask.shape}\")\n","    print(f\"segment_ids:{segment_ids.shape}\")\n","    print(f\"labels:{labels.shape}\")\n","\n","    print(f\"bert_inputs:{bert_inputs.keys()}\")\n","    print(f\"bert_outputs:{bert_outputs.keys()}\")\n","\n","    # Use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"sequence_output\"]\n","\n","    print(f\"output_layer shape:{output_layer.shape}\")\n","    print(f\"pooled_output shape:{bert_outputs['pooled_output'].shape}\")\n","    print(f\"sequence_output shape:{bert_outputs['sequence_output'].shape}\")\n","\n","    hidden_size = output_layer.shape[-1].value\n","    # num_labels = input_ids.shape[1]\n","    one_hot_labels = tf.one_hot(labels, hidden_size, name=\"My_OneHot\")\n","    print(f\"one_hot_labels shape:{one_hot_labels.shape}\")\n","\n","    with tf.variable_scope(\"loss\"):\n","        # Dropout helps prevent overfitting\n","        # output_layer = tf.nn.dropout(output_layer, keep_prob=0.9, name=\"My_Dropout\")\n","        # print(f\"1 output_layer shape:{output_layer.shape}\")\n","\n","        # output_layer = tf.nn.softmax(output_layer, name=\"My Softmax\")\n","        # print(f\"2 output_layer shape:{output_layer.shape}\")\n","\n","        softmax = tf.nn.softmax(output_layer, name=\"My_Softmax\")\n","        print(f\"2 softmax shape:{softmax.shape}\")\n","\n","        argmax = tf.math.argmax(softmax, axis=-1, name=\"My_Argmax\")\n","\n","        i = 6\n","\n","        print(f\"2 argmax shape:{argmax.shape}\")\n","        predicted_labels = argmax\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook(\n","                {\"argmax\": argmax[0, i],\n","                 \"label\": labels[0, i],\n","                 \"predicted\": predicted_labels[0, i],\n","                 \"OK labels\": tf.math.reduce_sum(\n","                     tf.to_float(tf.math.equal(tf.cast(labels, tf.int64), predicted_labels)))},\n","                every_n_iter=10))\n","\n","        if is_predicting:\n","            return predicted_labels\n","\n","        print(f\"logits : {output_layer.shape}\")\n","        print(f\"labels : {one_hot_labels.shape}\")\n","\n","        # output_layer = tf.reshape(output_layer, [BATCH_SIZE ,MAX_SEQ_LEN, 768])\n","        # print(f\"logits : {output_layer.shape}\")\n","        # print(f\"tmp : {tmp.shape}\")\n","\n","        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=output_layer, name=\"My_Loss\")\n","        loss = tf.reduce_mean(tf.math.multiply(loss, tf.to_float(input_mask), name=\"My_Multiply\"), name=\"My_ReduceMean\")\n","        print(f\"loss : {loss.shape}\")\n","\n","        return loss, predicted_labels\n","\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","\n","        print(f\"model_fn: input_ids:{input_ids.shape}\")\n","        print(f\"model_fn: input_mask:{input_mask.shape}\")\n","        print(f\"model_fn: segment_ids:{segment_ids.shape}\")\n","        print(f\"model_fn: label_ids:{label_ids.shape}\")\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","\n","            (loss, predicted_labels) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            hooks.append(tf.train.LoggingTensorHook({\"loss\": loss}, every_n_iter=10))\n","\n","            train_op = bert.optimization.create_optimizer(\n","                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            # Calculate evaluation metrics.\n","            def metric_fn(label_ids, predicted_labels):\n","                print(f\"metric label_ids {label_ids.shape}\")\n","                print(f\"metric predicted_labels {predicted_labels.shape}\")\n","\n","                y_true, y_pred = label_ids, predicted_labels\n","\n","                precision = tf_metrics.precision(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                recall = tf_metrics.recall(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                f2 = tf_metrics.fbeta(\n","                    y_true, y_pred, num_classes, pos_indices, average=average, beta=2)\n","                f1 = tf_metrics.f1(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","\n","                return {\n","                    \"precision\": precision,\n","                    \"recall\": recall,\n","                    \"f2\": f2,\n","                    \"f1\": f1\n","                }\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","                \n","                return tf.contrib.tpu.TPUEstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  train_op=train_op,\n","                                                  training_hooks=hooks)\n","            else:\n","                return tf.contrib.tpu.TPUEstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  eval_metric_ops=eval_metrics)\n","        else:\n","            (predicted_labels, log_probs) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            predictions = {\n","                'probabilities': log_probs,\n","                'labels': predicted_labels\n","            }\n","            return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)\n","\n","    # Return the actual model function in the closure\n","    return model_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wIc7srmyv8U","colab_type":"code","outputId":"6dc4743c-852c-48ab-e4db-c5bdbbf6d157","executionInfo":{"status":"ok","timestamp":1564835186628,"user_tz":180,"elapsed":2352,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#TPU REFACTORING: detect the TPU\n","try: # TPU detection\n","  tpu = tf.contrib.cluster_resolver.TPUClusterResolver() # Picks up a connected TPU on Google's Colab, ML Engine, Kubernetes and Deep Learning VMs accessed through the 'ctpu up' utility\n","  #tpu = tf.contrib.cluster_resolver.TPUClusterResolver('MY_TPU_NAME') # If auto-detection does not work, you can pass the name of the TPU explicitly (tip: on a VM created with \"ctpu up\" the TPU has the same name as the VM)\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","  USE_TPU = True\n","except ValueError:\n","  tpu = None\n","  print(\"Running on GPU or CPU\")\n","  USE_TPU = False"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Running on TPU  ['10.1.210.18:8470']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BT2TQBBe_PO2","colab_type":"code","outputId":"d108cb32-9a1f-4f43-ba1a-3dc5905cff0b","executionInfo":{"status":"error","timestamp":1564835449695,"user_tz":180,"elapsed":265392,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":547}},"source":["tf.logging.set_verbosity(tf.logging.DEBUG)\n","shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n","current_time = datetime.now()\n","# \"../../extra_files/bert_finetuning/\"\n","tokens, ids, labels = load_features(f\"{root}/prepared/bert_finetuning3\", MAX_EXAMPLES)\n","\n","NUM_TRAIN_EPOCHS = int(max([len(x) for x in labels])/5)\n","print(f'Beginning Training! {NUM_TRAIN_EPOCHS}')\n","\n","estimator = build_estimator(labels)\n","\n","for epoch in range(NUM_TRAIN_EPOCHS):\n","    print(f\"New Epoch {epoch}\")\n","    train_input_fn = slice_input(epoch, tokens, ids, labels, True)\n","    eval_input_fn = slice_input(epoch, tokens, ids, labels, False)\n","\n","    print(f\"hooks:{len(hooks)}\")\n","    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n","    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n","    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","\n","    # estimator.train(input_fn=slice_input(epoch, tokens, ids, labels, True), hooks=hooks)\n","    hooks.clear()\n","\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|██████████| 663/663 [04:16<00:00,  2.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Beginning Training! 800\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-0bbc51e4c4a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Beginning Training! {NUM_TRAIN_EPOCHS}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-0df546f08bda>\u001b[0m in \u001b[0;36mbuild_estimator\u001b[0;34m(labels)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         params={\"batch_size\": BATCH_SIZE})\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_fn, model_dir, config, params, use_tpu, train_batch_size, eval_batch_size, predict_batch_size, batch_axis, eval_on_tpu, export_to_tpu, export_to_cpu, warm_start_from, experimental_export_device_assignment, embedding_config_spec, export_saved_model_api_version)\u001b[0m\n\u001b[1;32m   2553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_RESERVED_PARAMS_KEYS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m       raise ValueError('{} are reserved keys but existed in params {}.'.format(\n\u001b[0;32m-> 2555\u001b[0;31m           _RESERVED_PARAMS_KEYS, params))\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: ['batch_size', 'context'] are reserved keys but existed in params {'batch_size': 8}."]}]},{"cell_type":"code","metadata":{"id":"mOLq6UhfPysq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"0f297c8e-d5bf-4944-9d1e-d05c394236e1","executionInfo":{"status":"ok","timestamp":1564835813371,"user_tz":180,"elapsed":735,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","todos = [len(x) for x in ids]\n","_ = plt.hist(np.clip(todos,0,500), bins=30)"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADapJREFUeJzt3VGMXGd5xvH/0zghFFBDksWyYtIN\nwgLlojHVKgSRCwgNcgkiuYgiEKK+cOUbkEBFok4rVULqhbkhUKmqapEIV6IkKRAlSlAhdYKqSlXA\nJgEcTBoTOWosJzY0AXpDa3h7McdhG9nM7Ozsrvfd/08azfm+843n/dbjZ4+/OWcmVYUkaf37nbUu\nQJI0Gwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSE5tW88kuv/zymp+fX82nlKR1\n79ChQz+pqrlx41Y10Ofn5zl48OBqPqUkrXtJnp1knEsuktSEgS5JTRjoktSEgS5JTRjoktTERGe5\nJDkG/AL4FXC6qhaSXArcA8wDx4DbqurFlSlTkjTOUo7Q311V26tqYWjvAQ5U1TbgwNCWJK2R5Sy5\n3AzsH7b3A7csvxxJ0rQmDfQCvpnkUJLdQ9/mqjoxbD8PbJ55dZKkiU16pej1VXU8yRuAh5P8aPHO\nqqokZ/226eEXwG6AK6+8cupC5/c8NNG4Y3tvmvo5JGk9m+gIvaqOD/cngfuAa4EXkmwBGO5PnuOx\n+6pqoaoW5ubGfhSBJGlKYwM9yWuSvO7MNvBe4DDwALBzGLYTuH+lipQkjTfJkstm4L4kZ8b/Y1X9\nc5LvAPcm2QU8C9y2cmVKksYZG+hV9QxwzVn6fwq8ZyWKkiQtnVeKSlITBrokNWGgS1ITBrokNWGg\nS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1IT\nBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrok\nNWGgS1ITBrokNTFxoCe5IMnjSR4c2lcleSzJ0ST3JLlo5cqUJI2zlCP0jwNHFrU/A9xRVW8GXgR2\nzbIwSdLSTBToSbYCNwFfGNoBbgC+MgzZD9yyEgVKkiYz6RH654BPAb8e2pcBL1XV6aH9HHDFjGuT\nJC3B2EBP8n7gZFUdmuYJkuxOcjDJwVOnTk3zR0iSJjDJEfo7gQ8kOQbczWip5fPAJUk2DWO2AsfP\n9uCq2ldVC1W1MDc3N4OSJUlnMzbQq+r2qtpaVfPAB4FHqurDwKPArcOwncD9K1alJGms5ZyH/ufA\nnyU5ymhN/c7ZlCRJmsam8UN+o6q+BXxr2H4GuHb2JUmSpuGVopLUhIEuSU0Y6JLUhIEuSU0Y6JLU\nhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEu\nSU0Y6JLUhIEuSU0s6UuiJUm/Mb/noYnGHdt70wpXMuIRuiQ1YaBLUhMGuiQ14Rr6Gjjf1t0k9eAR\nuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhNjAz3JxUm+neR7SZ5M8umh/6okjyU5\nmuSeJBetfLmSpHOZ5Aj9l8ANVXUNsB3YkeQ64DPAHVX1ZuBFYNfKlSlJGmdsoNfIfw/NC4dbATcA\nXxn69wO3rEiFkqSJTLSGnuSCJE8AJ4GHgR8DL1XV6WHIc8AVK1OiJGkSEwV6Vf2qqrYDW4FrgbdO\n+gRJdic5mOTgqVOnpixTkjTOks5yqaqXgEeBdwCXJDnzaY1bgePneMy+qlqoqoW5ubllFStJOrdJ\nznKZS3LJsP1q4EbgCKNgv3UYthO4f6WKlCSNN8nnoW8B9ie5gNEvgHur6sEkPwTuTvLXwOPAnStY\npyRpjLGBXlXfB952lv5nGK2nS5LOA14pKklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS\n1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISB\nLklNGOiS1ISBLklNGOiS1MSmtS5AG8P8nocmGnds700rXInUl0foktSEgS5JTRjoktSEgS5JTRjo\nktSEgS5JTRjoktTE2PPQk7wR+AdgM1DAvqr6fJJLgXuAeeAYcFtVvbhypU7G850lbVSTHKGfBj5Z\nVVcD1wEfTXI1sAc4UFXbgANDW5K0RsYGelWdqKrvDtu/AI4AVwA3A/uHYfuBW1aqSEnSeEtaQ08y\nD7wNeAzYXFUnhl3PM1qSkSStkYkDPclrga8Cn6iqny/eV1XFaH39bI/bneRgkoOnTp1aVrGSpHOb\nKNCTXMgozL9UVV8bul9IsmXYvwU4ebbHVtW+qlqoqoW5ublZ1CxJOouxgZ4kwJ3Akar67KJdDwA7\nh+2dwP2zL0+SNKlJPj73ncBHgB8keWLo+wtgL3Bvkl3As8BtK1OiJGkSYwO9qv4NyDl2v2e25UiS\npuWVopLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y\n6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU1sWusC\npK7m9zw00bhje29a4Uq0UXiELklNGOiS1ISBLklNbNg1dNc3JXXjEbokNWGgS1ITBrokNWGgS1IT\nYwM9yV1JTiY5vKjv0iQPJ3l6uH/9ypYpSRpnkiP0LwI7XtG3BzhQVduAA0NbkrSGxgZ6Vf0r8F+v\n6L4Z2D9s7wdumXFdkqQlmvY89M1VdWLYfh7YfK6BSXYDuwGuvPLKKZ9O0vlu0ms7wOs7Vsqy3xSt\nqgLqt+zfV1ULVbUwNze33KeTJJ3DtIH+QpItAMP9ydmVJEmaxrSB/gCwc9jeCdw/m3IkSdOa5LTF\nLwP/DrwlyXNJdgF7gRuTPA380dCWJK2hsW+KVtWHzrHrPTOuZd1byptCkjRrXikqSU0Y6JLUhIEu\nSU1s2C+4mJTr4pLWC4/QJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12S\nmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJv1N0A5n0+1GP7b1phSuR\ntBI8QpekJgx0SWrCQJekJgx0SWrCN0U1tUnfZJW0OjxCl6QmDHRJasJAl6QmlrWGnmQH8HngAuAL\nVbV3JlVpSTqtZa/VxU9edKUOpj5CT3IB8LfAHwNXAx9KcvWsCpMkLc1yllyuBY5W1TNV9T/A3cDN\nsylLkrRUywn0K4D/XNR+buiTJK2BVNV0D0xuBXZU1Z8O7Y8Ab6+qj71i3G5g99B8C/DUFE93OfCT\nqQpdv5zzxuCcN4blzvn3q2pu3KDlvCl6HHjjovbWoe//qap9wL5lPA9JDlbVwnL+jPXGOW8Mznlj\nWK05L2fJ5TvAtiRXJbkI+CDwwGzKkiQt1dRH6FV1OsnHgG8wOm3xrqp6cmaVSZKWZFnnoVfV14Gv\nz6iW32ZZSzbrlHPeGJzzxrAqc576TVFJ0vnFS/8lqYnzPtCT7EjyVJKjSfasdT2zkuSuJCeTHF7U\nd2mSh5M8Pdy/fuhPkr8ZfgbfT/KHa1f59JK8McmjSX6Y5MkkHx/62847ycVJvp3ke8OcPz30X5Xk\nsWFu9wwnFpDkVUP76LB/fi3rn1aSC5I8nuTBod16vgBJjiX5QZInkhwc+lb1tX1eB3rzjxf4IrDj\nFX17gANVtQ04MLRhNP9tw2038HerVOOsnQY+WVVXA9cBHx3+PjvP+5fADVV1DbAd2JHkOuAzwB1V\n9WbgRWDXMH4X8OLQf8cwbj36OHBkUbv7fM94d1VtX3SK4uq+tqvqvL0B7wC+sah9O3D7Wtc1w/nN\nA4cXtZ8CtgzbW4Cnhu2/Bz50tnHr+QbcD9y4UeYN/C7wXeDtjC4y2TT0v/w6Z3TW2DuG7U3DuKx1\n7Uuc51ZG4XUD8CCQzvNdNO9jwOWv6FvV1/Z5fYTOxvt4gc1VdWLYfh7YPGy3+zkM/7V+G/AYzec9\nLD88AZwEHgZ+DLxUVaeHIYvn9fKch/0/Ay5b3YqX7XPAp4BfD+3L6D3fMwr4ZpJDwxXysMqvbb+C\n7jxVVZWk5SlISV4LfBX4RFX9PMnL+zrOu6p+BWxPcglwH/DWNS5pxSR5P3Cyqg4ledda17PKrq+q\n40neADyc5EeLd67Ga/t8P0Kf6OMFGnkhyRaA4f7k0N/m55DkQkZh/qWq+trQ3X7eAFX1EvAooyWH\nS5KcOaBaPK+X5zzs/z3gp6tc6nK8E/hAkmOMPoH1BkbfmdB1vi+rquPD/UlGv7ivZZVf2+d7oG+0\njxd4ANg5bO9ktMZ8pv9PhnfGrwN+tui/cetGRofidwJHquqzi3a1nXeSueHInCSvZvSewRFGwX7r\nMOyVcz7zs7gVeKSGRdb1oKpur6qtVTXP6N/rI1X1YZrO94wkr0nyujPbwHuBw6z2a3ut30iY4I2G\n9wH/wWjd8S/Xup4ZzuvLwAngfxmtn+1itHZ4AHga+Bfg0mFsGJ3t82PgB8DCWtc/5ZyvZ7TO+H3g\nieH2vs7zBv4AeHyY82Hgr4b+NwHfBo4C/wS8aui/eGgfHfa/aa3nsIy5vwt4cCPMd5jf94bbk2ey\narVf214pKklNnO9LLpKkCRnoktSEgS5JTRjoktSEgS5JTRjoktSEgS5JTRjoktTE/wHNUanXWFHI\nHwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}
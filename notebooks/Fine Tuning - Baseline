{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine Tuning - Baseline","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JEPsT9Tf2SON","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"b9e0198f-d8a0-4db9-d59c-e4280c3e2f5b","executionInfo":{"status":"ok","timestamp":1570648692866,"user_tz":180,"elapsed":18333,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}}},"source":["from google.colab import drive\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","drive.mount('/content/gdrive/')\n","\n","root = r\"/content/gdrive/My Drive/Puc/Projeto Final/Datasets/bert\"\n","data_folder = \"train/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tzo7TJwKQ30y","colab_type":"text"},"source":["!pip install git+https://github.com/guillaumegenthial/tf_metrics.git\n","!pip install tensorflow-gpu --force-reinstall"]},{"cell_type":"code","metadata":{"id":"9W4yFaEZNkls","colab_type":"code","outputId":"eaf5b63a-2874-4480-98bf-1a8d4cb67dc1","executionInfo":{"status":"ok","timestamp":1564859018444,"user_tz":180,"elapsed":8996,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":564}},"source":["!pip install bert-tensorflow\n","!pip install git+https://github.com/guillaumegenthial/tf_metrics.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Collecting git+https://github.com/guillaumegenthial/tf_metrics.git\n","  Cloning https://github.com/guillaumegenthial/tf_metrics.git to /tmp/pip-req-build-b20gja2o\n","  Running command git clone -q https://github.com/guillaumegenthial/tf_metrics.git /tmp/pip-req-build-b20gja2o\n","Requirement already satisfied (use --upgrade to upgrade): tf-metrics==0.0.1 from git+https://github.com/guillaumegenthial/tf_metrics.git in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.16.4)\n","Requirement already satisfied: tensorflow-gpu>=1.6 in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.1.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.33.4)\n","Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.14.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.0.8)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.8.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (41.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.15.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.8.0)\n","Building wheels for collected packages: tf-metrics\n","  Building wheel for tf-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tf-metrics: filename=tf_metrics-0.0.1-cp36-none-any.whl size=7694 sha256=2a99f97d0b484f2a2ebb4709a77141a7f0f2f1100bf58792fbbd8ecfdd9eb7e3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-85fas4k4/wheels/da/6c/c8/663ef339a0666590dc53bd13bab86643a1f9c35b26742d7876\n","Successfully built tf-metrics\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LezJdDcDJWoi","colab_type":"code","outputId":"636b10a3-19aa-409e-cde1-93c6063048f2","executionInfo":{"status":"ok","timestamp":1564859024153,"user_tz":180,"elapsed":2094,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["import tensorflow as tf\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","\n","from datetime import datetime\n","import tensorflow_hub as hub\n","import shutil\n","import sys\n","import pickle\n","import tf_metrics\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0803 19:03:42.386044 139673787373440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sdaMPFg9JbBW","colab_type":"code","colab":{}},"source":["hooks = []\n","debug = {}\n","\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","# Compute train and warmup steps from batch size\n","BATCH_SIZE = 8\n","MAX_EXAMPLES = 2000\n","LEARNING_RATE = 2e-5\n","# Warmup is a period of time where hte learning rate\n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","OUTPUT_DIR = \"BERT\"\n","MAX_SEQ_LEN = 512\n","num_classes = 768\n","pos_indices = list(range(1, num_classes))\n","average = 'micro'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVoiG0YtJe-v","colab_type":"code","colab":{}},"source":["def build_estimator(labels):\n","    # Compute # train and warmup steps from batch size\n","    total_samples = len(labels)\n","    num_train_steps = int(total_samples / BATCH_SIZE)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","        num_labels=0,\n","        learning_rate=LEARNING_RATE,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","        model_fn=model_fn,\n","        config=run_config,\n","        params={\"batch_size\": BATCH_SIZE})\n","\n","    return estimator\n","\n","\n","def load_features(path, max_size=10000):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    if os.path.isfile(\"all_features.dmp\"):\n","        with open(\"all_features.dmp\", \"rb\")as f:\n","            labels, tokens, ids = pickle.load(f)\n","    else:\n","        for file_name in tqdm(sorted(list(os.listdir(path)))):\n","            df = pd.read_csv(f\"{path}/{file_name}\", header=None, names=list(range(MAX_SEQ_LEN)), index_col=None)\n","\n","            if file_name.endswith(\"y\"):\n","                # Changing -1 and 0\n","                df = df.apply(lambda row: [-(x + 1) if x <= 0 else x for x in row])\n","                # Moving all 1 to get all positive\n","                labels.append(df + 1)\n","            elif file_name.endswith(\"x1\"):\n","                tokens.append(df)\n","            elif file_name.endswith(\"x2\"):\n","                ids.append(df)\n","\n","            if len(tokens) >= max_size and \\\n","                    len(labels) >= max_size and \\\n","                    len(ids) >= max_size:\n","                break\n","\n","        with open(\"all_features.dmp\", \"wb\") as f:\n","            pickle.dump((labels, tokens, ids), f)\n","\n","    masks = [k.sum(axis=1).values > 0 for k in ids]\n","    for i in range(len(masks)):\n","        labels[i] = labels[i][masks[i]]\n","        tokens[i] = tokens[i][masks[i]]\n","        ids[i] = ids[i][masks[i]]\n","\n","    labels = [labels[i] for i in range(len(masks)) if len(labels[i]) > 0]\n","    tokens = [tokens[i] for i in range(len(masks)) if len(tokens[i]) > 0]\n","    ids = [ids[i] for i in range(len(masks)) if len(ids[i]) > 0]\n","\n","    return tokens, ids, labels\n","\n","\n","def slice_input(epoch, o_tokens, o_ids, o_labels, train=True):\n","    labels = []\n","    tokens = []\n","    ids = []\n","\n","    def slice(df):\n","        full = len(df) / 5\n","        chunk = int(epoch % full)\n","        # Doing a 80-20 train-test split\n","        slice_start = chunk * 5\n","        slice_end = slice_start + 4\n","\n","        if not train:\n","            slice_start = slice_end\n","            slice_end = slice_start + 1\n","\n","        return df.iloc[slice_start:slice_end, :]\n","\n","    for i in range(len(o_tokens)):\n","        labels.append(slice(o_labels[i]))\n","        tokens.append(slice(o_tokens[i]))\n","        ids.append(slice(o_ids[i]))\n","\n","    def fix_it(df):\n","        #print(f\"list values len {len(df)}\")\n","        #print(f\"shape 0 {df[0].shape}\")\n","        df = pd.concat(df)\n","        #print(f\"New df:{df.shape}\")\n","        return df.fillna(0).values.astype('int32')\n","\n","    sliced_labels = fix_it(labels)\n","    sliced_tokens = fix_it(tokens)\n","    sliced_ids = fix_it(ids)\n","    sliced_mask = (sliced_ids > 0).astype('int32')\n","\n","    return input_fn_builder(sliced_tokens, sliced_ids, sliced_labels, sliced_mask, train, False)\n","\n","\n","def input_fn_builder(tokens, ids, labels, mask, is_training, drop_remainder):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    print(f\"Tokens shape:{tokens.shape}\")\n","\n","    num_examples, seq_length = tokens.shape\n","\n","    print(f\"t {num_examples}, {seq_length}\")\n","\n","    all_input_ids = tokens\n","    all_input_mask = mask\n","    all_segment_ids = ids\n","    all_label_ids = labels\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # This is for demo purposes and does NOT scale to large data sets. We do\n","        # not use Dataset.from_generator() because that uses tf.py_func which is\n","        # not TPU compatible. The right way to load data is with TFRecordReader.\n","        d = tf.data.Dataset.from_tensor_slices({\n","            \"input_ids\": all_input_ids,\n","            \"input_mask\": all_input_mask,\n","            \"segment_ids\": all_segment_ids,\n","            \"label_ids\": all_label_ids\n","        })\n","\n","        if is_training:\n","            # d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","        d = d.prefetch(batch_size)\n","        return d\n","\n","    return input_fn\n","\n","\n","def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    bert_module = hub.Module(\n","        BERT_MODEL_HUB,\n","        trainable=True)\n","    bert_inputs = dict(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids)\n","    bert_outputs = bert_module(\n","        inputs=bert_inputs,\n","        signature=\"tokens\",\n","        as_dict=True)\n","\n","    #print(f\"input_ids:{input_ids.shape}\")\n","    #print(f\"input_mask:{input_mask.shape}\")\n","    #print(f\"segment_ids:{segment_ids.shape}\")\n","    #print(f\"labels:{labels.shape}\")\n","\n","    #print(f\"bert_inputs:{bert_inputs.keys()}\")\n","    #print(f\"bert_outputs:{bert_outputs.keys()}\")\n","\n","    # Use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"sequence_output\"]\n","    \n","    #brick = np.zeros(num_classes)\n","    #brick[1] = 1\n","    #wall = np.reshape(np.tile(brick, MAX_SEQ_LEN), (MAX_SEQ_LEN, num_classes))\n","    #wall = tf.convert_to_tensor(wall, dtype=tf.float32)\n","    #output_layer = tf.broadcast_to(wall, tf.shape(bert_outputs[\"sequence_output\"]))\n","\n","\n","    #print(f\"output_layer shape:{output_layer.shape}\")\n","    #print(f\"pooled_output shape:{bert_outputs['pooled_output'].shape}\")\n","    #print(f\"sequence_output shape:{bert_outputs['sequence_output'].shape}\")\n","\n","    hidden_size = output_layer.shape[-1].value\n","    # num_labels = input_ids.shape[1]\n","    one_hot_labels = tf.one_hot(labels, hidden_size, name=\"My_OneHot\")\n","    #print(f\"one_hot_labels shape:{one_hot_labels.shape}\")\n","\n","    with tf.variable_scope(\"loss\"):\n","        # Dropout helps prevent overfitting\n","        # output_layer = tf.nn.dropout(output_layer, keep_prob=0.9, name=\"My_Dropout\")\n","        # print(f\"1 output_layer shape:{output_layer.shape}\")\n","\n","        # output_layer = tf.nn.softmax(output_layer, name=\"My Softmax\")\n","        # print(f\"2 output_layer shape:{output_layer.shape}\")\n","\n","        # tmp_mask = tf.broadcast_to(tf.expand_dims(input_mask, axis=-1), tf.shape(output_layer))\n","        # output_layer = tf.math.multiply(output_layer, tf.cast(tmp_mask, tf.float32), name=\"My_Argmax_Mask\")\n","\n","        softmax = tf.nn.softmax(output_layer, name=\"My_Softmax\")\n","\n","        argmax = tf.math.argmax(output_layer, axis=-1, name=\"My_Argmax\")\n","\n","        predicted_labels = tf.math.multiply(argmax, tf.cast(input_mask, tf.int64), name=\"My_Argmax_Mask\")\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook(\n","                {\"softmax\": softmax[0],\n","                 \"label\": labels[0],\n","                 \"argmax\": argmax[0][0],\n","                 \"output_layer\": output_layer[0][0],\n","                 \"predicted\": predicted_labels[0],\n","                 \"OK labels\": tf.math.reduce_sum(\n","                     tf.to_float(tf.math.equal(tf.cast(labels, tf.int64), predicted_labels))),\n","                 \"Total Labels\": tf.shape(labels),\n","                 \"softmax shape\": tf.shape(softmax),\n","                 \"argmax shape\" : tf.shape(argmax),\n","                 \"predicted_labels shape\" : tf.shape(predicted_labels),\n","                 \"input_mask shape\": tf.shape(input_mask)},\n","                every_n_iter=10))\n","\n","        if is_predicting:\n","            return predicted_labels\n","\n","        #print(f\"logits : {output_layer.shape}\")\n","        #print(f\"labels : {one_hot_labels.shape}\")\n","\n","        # output_layer = tf.reshape(output_layer, [BATCH_SIZE ,MAX_SEQ_LEN, 768])\n","        # print(f\"logits : {output_layer.shape}\")\n","        # print(f\"tmp : {tmp.shape}\")\n","\n","        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=output_layer, name=\"My_Loss\")\n","        no_loss = tf.math.multiply(loss, tf.to_float(input_mask), name=\"My_Multiply\")\n","        weightened = tf.divide(no_loss, tf.cast(tf.reduce_sum(input_mask),tf.float32))\n","        rm = tf.reduce_sum(weightened)\n","\n","        hooks.append(\n","            tf.estimator.LoggingTensorHook({\n","                \"count no_loss\": tf.math.count_nonzero(loss),\n","                \"count input_mask\": tf.math.count_nonzero(input_mask),\n","                \"loss shape\": tf.shape(loss),\n","                \"rm shape\": tf.shape(rm),\n","                \"no_loss\": no_loss[0],\n","                \"weightened\": weightened[0],\n","                \"loss\": loss[0],\n","                \"no_loss shape\": tf.shape(no_loss),\n","                \"one_hot_labels shape\": tf.shape(one_hot_labels)                \n","            }, every_n_iter=10))\n","\n","        return rm, predicted_labels\n","\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","\n","        #print(f\"model_fn: input_ids:{input_ids.shape}\")\n","        #print(f\"model_fn: input_mask:{input_mask.shape}\")\n","        #print(f\"model_fn: segment_ids:{segment_ids.shape}\")\n","        #print(f\"model_fn: label_ids:{label_ids.shape}\")\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","\n","            (loss, predicted_labels) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            hooks.append(tf.train.LoggingTensorHook({\"loss\": loss}, every_n_iter=10))\n","\n","            train_op = bert.optimization.create_optimizer(\n","                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            # Calculate evaluation metrics.\n","            def metric_fn(label_ids, predicted_labels):\n","                #print(f\"metric label_ids {label_ids.shape}\")\n","                #print(f\"metric predicted_labels {predicted_labels.shape}\")\n","\n","                y_true, y_pred = label_ids, predicted_labels\n","\n","                precision = tf_metrics.precision(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                recall = tf_metrics.recall(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","                f2 = tf_metrics.fbeta(\n","                    y_true, y_pred, num_classes, pos_indices, average=average, beta=2)\n","                f1 = tf_metrics.f1(\n","                    y_true, y_pred, num_classes, pos_indices, average=average)\n","\n","                return {\n","                    \"precision\": precision,\n","                    \"recall\": recall,\n","                    \"f2\": f2,\n","                    \"f1\": f1\n","                }\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","                \n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  train_op=train_op,\n","                                                  training_hooks=hooks)\n","            else:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  eval_metric_ops=eval_metrics)\n","        else:\n","            (predicted_labels, log_probs) = create_model(\n","                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            predictions = {\n","                'probabilities': log_probs,\n","                'labels': predicted_labels\n","            }\n","            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","    # Return the actual model function in the closure\n","    return model_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT2TQBBe_PO2","colab_type":"code","outputId":"d8fe4020-19b1-43e2-d0cb-37479ea4f796","executionInfo":{"status":"error","timestamp":1564859074365,"user_tz":180,"elapsed":15033,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["# tf.logging.set_verbosity(tf.logging.DEBUG)\n","shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n","current_time = datetime.now()\n","# \"../../extra_files/bert_finetuning/\"\n","tokens, ids, labels = load_features(f\"{root}/prepared/bert_finetuning3\", MAX_EXAMPLES)\n","\n","NUM_TRAIN_EPOCHS = int(max([len(x) for x in labels])/5)\n","print(f'Beginning Training! {NUM_TRAIN_EPOCHS}')\n","\n","estimator = build_estimator(labels)\n","\n","for epoch in range(NUM_TRAIN_EPOCHS):\n","    print(f\"New Epoch {epoch}\")\n","    train_input_fn = slice_input(epoch, tokens, ids, labels, True)\n","    eval_input_fn = slice_input(epoch, tokens, ids, labels, False)\n","\n","    #print(f\"hooks:{len(hooks)}\")\n","    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n","    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n","    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","\n","    # estimator.train(input_fn=slice_input(epoch, tokens, ids, labels, True), hooks=hooks)\n","    hooks.clear()\n","\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-23b4c5184aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# \"../../extra_files/bert_finetuning/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{root}/prepared/bert_finetuning3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_EXAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mNUM_TRAIN_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-f54fdc6c1a3e>\u001b[0m in \u001b[0;36mload_features\u001b[0;34m(path, max_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2916\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2965\u001b[0m         \u001b[0;31m# check_bool_indexer will throw exception if Series key cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m         \u001b[0;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2967\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2968\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"JrGj4ccIfAj9","colab_type":"code","colab":{}},"source":["FINAL_DEST = f\"{root}/trainded\"\n","shutil.rmtree(FINAL_DEST, ignore_errors=True)\n","shutil.copytree(OUTPUT_DIR, FINAL_DEST)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mOLq6UhfPysq","colab_type":"code","outputId":"5de90653-3622-4fd5-ef05-0daa764da8e2","executionInfo":{"status":"ok","timestamp":1565206927897,"user_tz":180,"elapsed":1027,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K3U6-YlOd5Xk","colab_type":"code","outputId":"3ea146d1-74b3-4e37-f00d-0ffee5f5c215","executionInfo":{"status":"ok","timestamp":1565206933124,"user_tz":180,"elapsed":4101,"user":{"displayName":"Leonardo Oliveira","photoUrl":"","userId":"02805186391476525049"}},"colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["%tensorboard --logdir baseline:/content/gdrive/My\\ Drive/Puc/Projeto\\ Final/Datasets/bert/baseline,trained:/content/gdrive/My\\ Drive/Puc/Projeto\\ Final/Datasets/bert/trained"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div id=\"root\"></div>\n","    <script>\n","      (function() {\n","        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n","        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n","        document.querySelector(\"base\").href = \"https://localhost:6006\";\n","        function fixUpTensorboard(root) {\n","          const tftb = root.querySelector(\"tf-tensorboard\");\n","          // Disable the fragment manipulation behavior in Colab. Not\n","          // only is the behavior not useful (as the iframe's location\n","          // is not visible to the user), it causes TensorBoard's usage\n","          // of `window.replace` to navigate away from the page and to\n","          // the `localhost:<port>` URL specified by the base URI, which\n","          // in turn causes the frame to (likely) crash.\n","          tftb.removeAttribute(\"use-hash\");\n","        }\n","        function executeAllScripts(root) {\n","          // When `script` elements are inserted into the DOM by\n","          // assigning to an element's `innerHTML`, the scripts are not\n","          // executed. Thus, we manually re-insert these scripts so that\n","          // TensorBoard can initialize itself.\n","          for (const script of root.querySelectorAll(\"script\")) {\n","            const newScript = document.createElement(\"script\");\n","            newScript.type = script.type;\n","            newScript.textContent = script.textContent;\n","            root.appendChild(newScript);\n","            script.remove();\n","          }\n","        }\n","        function setHeight(root, height) {\n","          // We set the height dynamically after the TensorBoard UI has\n","          // been initialized. This avoids an intermediate state in\n","          // which the container plus the UI become taller than the\n","          // final width and cause the Colab output frame to be\n","          // permanently resized, eventually leading to an empty\n","          // vertical gap below the TensorBoard UI. It's not clear\n","          // exactly what causes this problematic intermediate state,\n","          // but setting the height late seems to fix it.\n","          root.style.height = `${height}px`;\n","        }\n","        const root = document.getElementById(\"root\");\n","        fetch(\".\")\n","          .then((x) => x.text())\n","          .then((html) => void (root.innerHTML = html))\n","          .then(() => fixUpTensorboard(root))\n","          .then(() => executeAllScripts(root))\n","          .then(() => setHeight(root, 800));\n","      })();\n","    </script>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"WF_94dr7YP9-","colab_type":"code","colab":{}},"source":["f\"{root}/trainded\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xxqh2L4T71_j","colab_type":"code","colab":{}},"source":["!kill 5351"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPznHbcuNHrH","colab_type":"code","colab":{}},"source":["!mv Baseline /content/gdrive/My\\ Drive/Puc/Projeto\\ Final/Datasets/bert/baseline"],"execution_count":0,"outputs":[]}]}